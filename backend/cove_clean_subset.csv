id_num,name,year,author,tasks,topics,keywords,types,annotations,size,num_cat,description,url,thumbnail,paper,citations,conferences,institutions,
0,20bn-Something-Something,2017,"Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski,Joanna Materzy?ska, Susanne Westphal, Heuna Kim,Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax,Roland Memisevic",classification,action,,video,category labels,108499,174,The 20BN-SOMETHING-SOMETHING dataset is a large collection of densely-labeled video clips that show humans performing pre-defined basic actions with everyday objects.,https://www.twentybn.com/datasets/something-something,https://20bn-frontend-production.s3.amazonaws.com/uploads/video/file/54650/large_b8362a7b34250b48c9c9.mp4,"The ""something something"" video database for learning and evaluating visual common sense","Goyal, R., Kahou, S. E., Michalski, V., Materzynska, J., Westphal, S., Kim, H., ... & Hoppe, F. (2017, June). The” something something” video database for learning and evaluating visual common sense. In Proc. ICCV.",ICCV,Twenty Billion Neurons,
1,Online RGBD Action Dataset (ORGBD),2014,"Gang Yu, Zicheng Liu, Junsong Yuan",classification;recognition,action,orderlet,RGB-D video;images;depth images;skeleton images,temporal category labels;bounding boxes,,7,"The dataset targets for human aciton (human-object interaction) recognition based on RGBD video data. There are seven categories of human actions: Drinking, eating, using laptop, reading cellphone, making phone call, reading book, using remote.",https://sites.google.com/site/skicyyu/rgbd_recognition,https://sites.google.com/site/skicyyu/_/rsrc/1412222840358/orgbd/illustration.png,Discriminative Orderlet Mining For Real-time Recognition of Human-Object Interaction,"Yu, G., Liu, Z., & Yuan, J. (2014, November). Discriminative orderlet mining for real-time recognition of human-object interaction. In Asian Conference on Computer Vision (pp. 50-65). Springer, Cham.",ACCV,Nanyang Technological University;Microsoft Research,
2,50 Salads,2013,S. Stein and S. J. McKenna,classification,action,food,video;depth images;sensor recordings,3D bounding boxes,,,"50 Salads dataset captures 25 people preparing 2 mixed salads each and contains over 4h of annotated accelerometer and RGB-D video data. Including detailed annotations, multiple sensor types, and two sequences per participant, the 50 Salads dataset may be used for research in areas such as activity recognition, activity spotting, sequence analysis, progress tracking, sensor fusion, transfer learning, and user-adaptation.",http://cvip.computing.dundee.ac.uk/datasets/foodpreparation/50salads/,http://cvip.computing.dundee.ac.uk/datasets/foodpreparation/50salads/rgb-sample.avi,User-adaptive models for recognizing food preparation activities,"Heilbron, F. C., & Niebles, J. C. (2014, April). Collecting and annotating human activities in web videos. In Proceedings of International Conference on Multimedia Retrieval (p. 377). ACM.",,University of Dundee,
3,ActivityNet-100,2015,,classification;recognition,action,,video,temporal category labels,19994,100,A Large-Scale Video Benchmark for Human Activity Understanding (old version),http://activity-net.org/index.html,http://img.youtube.com/vi/Ou24uqaFRPg/1.jpg,,"Heilbron, F. C., & Niebles, J. C. (2014, April). Collecting and annotating human activities in web videos. In Proceedings of International Conference on Multimedia Retrieval (p. 377). ACM.",CVPR,"VCC Kaust;Universidad del Norte

",
4,ActivityNet-200,2016,,classification;recognition,action,,video,temporal category labels,9682,200,"A Large-Scale Video Benchmark for Human Activity Understanding (200 classes, 100 videos per class, 648 video hours)",http://activity-net.org/index.html,http://img.youtube.com/vi/-YjGbsbDoxs/1.jpg,,,CVPR,"VCC Kaust;Universidad del Norte

",
5,MERL Shopping Dataset,2016," Michael Jones, Tim Marks",detection,action,,video,temporal category labels,106,5," MERL Shopping Dataset consists of 106 videos, each of which is a sequence about 2 minutes long. The videos are from a fixed overhead camera looking down at people shopping in a grocery store setting. Each video contains several instances of the following 5 actions: ""Reach To Shelf"" (reach hand into shelf), ""Retract From Shelf "" (retract hand from shelf), ""Hand In Shelf"" (extended period with hand in the shelf), ""Inspect Product"" (inspect product while holding it in hand), and ""Inspect Shelf"" (look at shelf while not touching or reaching for the shelf).",http://www.merl.com/demos/merl-shopping-dataset,http://www.merl.com/public/img/photography/merlshoppingdataset-banner.jpg,A Multi-Stream Bi-Directional Recurrent Neural Network for Fine-Grained Action Detection,"Singh, B., Marks, T.K., Jones, M.J, , Tuzel, C.O., ""A Multi-Stream Bi-Directional Recurrent Neural Network for Fine-Grained Action Detection"", IEEE Conference on Computer Vision and Pattern Recognition (CVPR), DOI: 10.1109/CVPR.2016.216, June 2016, pp. 1961-1970.",CVPR,University of Maryland;MERL;Northeastern University,
6,Actor and Action Dataset ,2015,Chenliang Xu and Jason J. Corso ,detection,action,,video,temporal category labels,3782,8,"We have collected a new dataset consisting of 3782 videos from YouTube; these videos are hence unconstrained in-the-wild videos with varying characteristics. We select seven classes of actors performing eight different actions. Our choice of actors covers articulated ones, such as adult, baby, bird, cat and dog, as well as rigid ones, such as ball and car. The eight actions are climbing, crawling, eating, flying, jumping, rolling, running, and walking. A single action class can be performed by various actors, but none of the actors can perform all eight actions.",http://web.eecs.umich.edu/~jjcorso/r/a2d/,http://web.eecs.umich.edu/~jjcorso/r/a2d/files/dataset_montage.jpg,Can humans fly? Action understanding with multiple classes of actors,"C. Xu, S.-H. Hsieh, C. Xiong, and J. J. Corso. Can humans fly? Action understanding with multiple classes of actors. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2015;C. Xu and J. J. Corso. Actor-action semantic segmentation with grouping-process models. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2016;Y. Yan, C. Xu, D. Cai, and J. J. Corso. Weakly supervised actor-action segmentation via robust multi-task ranking. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2017",CVPR,University of Michigan,
7,Action Similarity Labeling (ASLAN) Challenge,2012,"Orit Kliper-Gross, Tal Hassner, and Lior Wolf",recognition,action,similarity,video,temporal category labels,3697,432,"The ASLAN set contains 3697 action samples from 1571 unique YouTube videos divided into 432 non-trivial action categories. An ""action sample"" is defined as a sub-sequence of a shot presenting a detected action, that is, a consecutive set of frames taken by the same camera presenting one action. The action samples have been manually labeled with the name of the action carried out in each of them. 316 of the categories contain more than one sample.",https://www.openu.ac.il/home/hassner/data/ASLAN/ASLAN.html,,The Action Similarity Labeling Challenge,"Kliper-Gross, O., Hassner, T., & Wolf, L. (2012). The action similarity labeling challenge. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(3), 615-621.",CVPR,"Weizmann Institute of Science;Open University of Israel;Tel Aviv
University",
8,Attribute Learning for Understanding Unstructured Social Activity,2012,"Yanwei Fu, Timothy M. Hospedales, Tao Xiang, and Shaogang Gong",classification,action,social events,video,gaze fixation annotations,9396,69,"he USAA dataset includes 8 different semantic class videos which are home videos of social occassions such e birthday party, graduation party,music performance, non-music performance, parade, wedding ceremony, wedding dance and wedding reception which feature activities of group of people. It contains around 100 videos for training and testing respectively. Each video is labeled by 69 attributes. The 69 attributes can be broken down into five broad classes: actions, objects, scenes, sounds, and camera movement.  It can be used for evaluating approaches for video classification,  N-shot and zero-shot learning, multi-task learning, attribute/concept-annotation, attribute/concepts-modality prediction, suprising attributes/concepts discovery, and latent-attribute(concepts) discovery etc.",http://yanweifu.github.io/USAA/download/,http://yanweifu.github.io/USAA/download/images/teaser_img.jpg,Attribute Learning for Understanding Unstructured Social Activity,";Fu, Y., Hospedales, T. M., Xiang, T., & Gong, S. (2012, October). Attribute learning for understanding unstructured social activity. In European Conference on Computer Vision (pp. 530-543). Springer, Berlin, Heidelberg.",ECCV,Queen Mary University of London,
9,BEHAVE Interacting Person Video Data with markup,2009,"S. J. Blunsden, R. B. Fisher",classification,action,,video;video frames,temporal category labels;bounding boxes,90000,6,"The BEHAVE project’s dataset has around 90,000 frames of humans identified by bounding boxes, with interacting groups classified into one of 6 different behaviors.",http://groups.inf.ed.ac.uk/vision/BEHAVEDATA/INTERACTIONS/index.html,http://groups.inf.ed.ac.uk/vision/BEHAVEDATA/INTERACTIONS/BEHAVE_fight.jpg,The BEHAVE video dataset: ground truthed video for multi-person behavior classification,"Blunsden, S., & Fisher, R. B. (2010). The BEHAVE video dataset: ground truthed video for multi-person behavior classification. Annals of the BMVA, 4(1-12), 4.",,University of Edinburgh,
10,BU-Action 101 ,2015,"S. Ma, S. A. Bargal, J. Zhang, L. Sigal, S. Sclaroff",classification,action,,images,category labels,23800,101,"The dataset consists of ~23.8K action images that correspond to the 101 action classes in the UCF101 video dataset. The action categories are divided into five types: Human-Object Interaction, Body-Motion Only, Human-Human Interaction, Playing Musical Instruments, Sports. For each action class, we automatically download images from the Web (Google, Flickr, etc.) using corresponding key phrases, e.g. pushup training for the class pushup, and then manually remove irrelevant images or drawings and cartoons. We also include 2769 images of relevant actions from the Standford40 dataset. Each class has at least 100 images and most classes have 150-300 images. ",http://cs-people.bu.edu/sbargal/BU-action/,http://cs-people.bu.edu/sbargal/BU-action/images/sample_images_small.png,Do Less and Achieve More: Training CNNs for Action Recognition Utilizing Action Images from the Web.,"Ma, S., Bargal, S. A., Zhang, J., Sigal, L., & Sclaroff, S. (2017). Do less and achieve more: Training cnns for action recognition utilizing action images from the web. Pattern Recognition, 68, 334-345.",,Boston University,
11,BU-Action 101 (unfiltered) ,2016,"S. Ma, S. A. Bargal, J. Zhang, L. Sigal, S. Sclaroff",classification,action,,images,category labels,204000,101,"This is a crawled dataset of web action images. It consists of ~204K images, with an average number of 2017 images per class. It is in one-to-one correspondence with the classes of the UCF101 video action dataset. The action categories are divided into five types: Human-Object Interaction, Body-Motion Only, Human-Human Interaction, Playing Musical Instruments, Sports. These crawled images are not manually labeled; we refer to them as unfiltered images. For each action class, we automatically download images from the Web (Google, Flickr, etc.) using corresponding key phrases, e.g. pushup training for the class pushup.",http://cs-people.bu.edu/sbargal/BU-action/,http://cs-people.bu.edu/sbargal/BU-action/images/sample_images_small.png,Do Less and Achieve More: Training CNNs for Action Recognition Utilizing Action Images from the Web.,"Ma, S., Bargal, S. A., Zhang, J., Sigal, L., & Sclaroff, S. (2017). Do less and achieve more: Training cnns for action recognition utilizing action images from the web. Pattern Recognition, 68, 334-345.",,Boston University,
12,BU-Action 203 ,2017,"S. Ma, S. A. Bargal, J. Zhang, L. Sigal, S. Sclaroff",classification,action,,images,category labels,387000,203,"This is a crawled dataset of web action images. It consists of ?387K images, with an average number of 1909 images per class. It is in one-to-one correspondence with the classes of the ActivityNet video action dataset. The action categories are divided into these main types: Personal Care, Working, Eating and Drinking, Socializing and Leisure, Household, Sports and Excercises, Caring and Helping. These crawled images are not manually labeled; we refer to them as unfiltered images. For each action class, we automatically download images from the Web (Google, Flickr, etc.) using corresponding key phrases, e.g. pushup training for the class pushup. ",http://cs-people.bu.edu/sbargal/BU-action/,http://cs-people.bu.edu/sbargal/BU-action/images/sample_images_small.png,Do Less and Achieve More: Training CNNs for Action Recognition Utilizing Action Images from the Web.,"Ma, S., Bargal, S. A., Zhang, J., Sigal, L., & Sclaroff, S. (2017). Do less and achieve more: Training cnns for action recognition utilizing action images from the web. Pattern Recognition, 68, 334-345.",,Boston University,
13,Berkeley MHAD: A Comprehensive Multimodal Human Action Database ,2013,"F. Ofli, R. Chaudhry, G. Kurillo, R. Vidal and R. Bajcsy",classification,action,,video;audio;sensor recordings;depth images,category labels;skeleton annotations,660,11,"The Berkeley Multimodal Human Action Database (MHAD) contains 11 actions performed by 7 male and 5 female subjects in the range 23-30 years of age except for one elderly subject. All the subjects performed 5 repetitions of each action, yielding about 660 action sequences which correspond to about 82 minutes of total recording time. In addition, we have recorded a T-pose for each subject which can be used for the skeleton extraction; and the background data (with and without the chair used in some of the activities).",http://tele-immersion.citris-uc.org/berkeley_mhad,http://tele-immersion.citris-uc.org/sites/default/files/action_samples_surface_800.jpg, Berkeley MHAD: A Comprehensive Multimodal Human Action Database,"F. Ofli, R. Chaudhry, G. Kurillo, R. Vidal and R. Bajcsy. Berkeley MHAD: A Comprehensive Multimodal Human Action Database. In Proceedings of the IEEE Workshop on Applications on Computer Vision (WACV), 2013.",WACV,"University of California, Berkeley;Johns Hopkins University",
14,Breakfast dataset ,2014,"H. Kuehne, A. B. Arslan and T. Serre",classification,action,food,video;video frames,temporal category labels,4000000,10,"This dataset comprises of 10 actions related to breakfast preparation, performed by 52 different individuals in 18 different kitchens.",http://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/,http://serre-lab.clps.brown.edu/wp-content/uploads/2012/04/example_breakfast_data-300x197.jpg,The Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities,"H. Kuehne, A. B. Arslan and T. Serre. The Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities. CVPR, 2014.;H. Kuehne, J. Gall and T. Serre. An end-to-end generative framework for video segmentation and recognition. WACV, 2016.",CVPR;WACV,Fraunhofer FKIE;Brown University,
15,Bristol Egocentric Object Interactions Dataset ,2014,"Dima Damen, Teesid Leelasawassuk, Osian Haines, Michael Wray, Davide Moltisanti, Andrew Calway, Walterio Mayol-Cuevas",gaze estimation,action,,video,gaze fixation annotations,58,6,Contains videos shot from a first-person (egocentric) point of view of 3-5 users performing tasks in six different locations.,https://data.bris.ac.uk/data/dataset/o4hx7jnmfqt01lyzf2n4rchg6,,"You-Do, I-Learn: Discovering Task Relevant Objects and their Modes of Interaction from Multi-User Egocentric Video","Damen, D., Leelasawassuk, T., Haines, O., Calway, A., & Mayol-Cuevas, W. W. (2014, September). You-Do, I-Learn: Discovering Task Relevant Objects and their Modes of Interaction from Multi-User Egocentric Video. In BMVC (Vol. 2, p. 3).",BMVC;ECCV,University of Bristol,
16,CAD-120 dataset ,2013,"Hema S Koppula, Rudhir Gupta, Ashutosh Saxena",detection,action,,RGB-D video;images;depth images,3D bounding boxes,60,32,The CAD-60 and CAD-120 data sets comprise of RGB-D video sequences of humans performing activities which are recording using the Microsoft Kinect sensor.,http://pr.cs.cornell.edu/humanactivities/,http://pr.cs.cornell.edu/humanactivities/images/all_activity_pic_combined.jpg,Learning human activities and object affordances from rgb-d videos,"Koppula, H. S., Gupta, R., & Saxena, A. (2013). Learning human activities and object affordances from rgb-d videos. The International Journal of Robotics Research, 32(8), 951-970.",,Cornell University,
17,CAD-60 dataset,2011,"Jaeyong Sung, Colin Ponce, Bart Selman, Ashutosh Saxena",detection,action,,RGB-D video;images;depth images,3D bounding boxes,120,12,The CAD-60 and CAD-120 data sets comprise of RGB-D video sequences of humans performing activities which are recording using the Microsoft Kinect sensor.,http://pr.cs.cornell.edu/humanactivities/,http://pr.cs.cornell.edu/humanactivities/images/all_activity_pic_combined.jpg,Unstructured human activity detection from rgbd images,"Sung, J., Ponce, C., Selman, B., & Saxena, A. (2012, May). Unstructured human activity detection from rgbd images. In Robotics and Automation (ICRA), 2012 IEEE International Conference on (pp. 842-849). IEEE.",ICRA,Cornell University,
18,Charades Dataset ,2017,"Sigurdsson, G. A., Varol, G., Wang, X., Farhadi, A., Laptev, I., & Gupta, A. ",detection;classification,action,,video,textual descriptions;category labels,9848,203,"The Charades Activity Challenge aims towards automatic understanding of daily activities, by providing realistic videos of people doing everyday activities. The Charades dataset is collected for an unique insight into daily tasks such as drinking coffee, putting on shoes while sitting in a chair, or snuggling with a blanket on the couch while watching something on a laptop. This enables computer vision algorithms to learn from real and diverse examples of our daily dynamic scenarios.  Instead
of shooting videos in the lab, we ensure diversity by distributing
and crowdsourcing the whole process of video creation from script writing
to video recording and annotation. Following this procedure we collect a
new dataset, Charades, with hundreds of people recording videos in their
own homes, acting out casual everyday activities. The dataset is composed
of 9,848 annotated videos with an average length of 30 seconds,
showing activities of 267 people from three continents, and over 15%
of the videos have more than one person. Each video is annotated by
multiple free-text descriptions, action labels, action intervals and classes
of interacted objects. In total, Charades provides 27,847 video descriptions,
66,500 temporally localized intervals for 157 action classes and
41,104 labels for 46 object classes",http://allenai.org/plato/charades/,,Hollywood in homes: Crowdsourcing data collection for activity understandin,"Sigurdsson, G. A., Varol, G., Wang, X., Farhadi, A., Laptev, I., & Gupta, A. (2016, October). Hollywood in homes: Crowdsourcing data collection for activity understanding. In European Conference on Computer Vision (pp. 510-526). Springer, Cham.",EECV;CVPR,Carnegie Mellon University;Inria;University of Washington;The Allen Institute of AI,
19,Composable Activities,2014,"Ivan Lillo, Alvaro Soto, Juan Carlos Niebles",classification,action,,RGB-D video,temporal category labels,693,16,"New dataset composed of 693 videos that contain activities in 16 classes performed by 14 actors. Each activity is composed of 3 to 11 atomic actions. We capture RGB-D data for each sequence using a Microsoft Kinect sensor and estimate position of relevant body joints.

We provide annotations of the activity for each video and the actions for each of the four human parts (left/right arm and leg) for each frame in every video.",http://ialillo.sitios.ing.uc.cl/ActionsCVPR2014/,http://ialillo.sitios.ing.uc.cl/ActionsCVPR2014/images/fig01.jpg,Discriminative hierarchical modeling of spatio-temporally composable human activities,"Lillo, I., Soto, A., & Carlos Niebles, J. (2014). Discriminative hierarchical modeling of spatio-temporally composable human activities. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 812-819).",CVPR,P. Universidad Catolica de Chile;Universidad del Norte. Colombia,
20,Depth-included Human Action (DHA) ,2012,"an-Ching Lin, Min-Chun Hu, Wen-Huang Cheng, Yung-Huan Hsieh, and Hong-Ming Chen",classification,action,,images;depth images,category labels,483,23,"DHA is containing 23 action categories: (1) bend, (2) jack, (3) jump, (4) pjump, (5) run, (6) side, (7) skip, (8) walk, (9) one-hand-wave, (10) two-hand-wave, (11) front-clap, (12) side-clap, (13) arm-swing, (14) arm-curl, (15) leg-kick, (16) leg-curl, (17) rod-swing, (18) golf-swing, (19) front-box, (20) side-box, (21) tai-chi, (22) pitch, (23) kick. The first 10 categories follow the same definitions in the Weizmann action dataset and the 11th to 16th actions are extended categories. The 17th to 23rd are the categories of selected sport actions.",http://mclab.citi.sinica.edu.tw/dataset/dha/dha.html,http://mclab.citi.sinica.edu.tw/dataset/dha/pic/RGB.jpg,Human action recognition and retrieval using sole depth information,"Lin, Y. C., Hu, M. C., Cheng, W. H., Hsieh, Y. H., & Chen, H. M. (2012, October). Human action recognition and retrieval using sole depth information. In Proceedings of the 20th ACM international conference on Multimedia (pp. 1053-1056). ACM.",MM,Academia Sinica,
21,DogCentric Activity Dataset,2014,"Y. Iwashita, A. Takamine, R. Kurazume, and M. S. Ryoo",classification,action;animal,dog,video,category labels,209,10,"We attached a GoPro camera to the back of each of the four dogs, and their owners took them on a walk to their familiar walking routes. The walking routes are in various environments, such as residential area, a park along a river, a sand beach, a field, streets with traffic, etc. Thus even though different dogs do the same activity, their background varies. 

The video contains various activities, and we chose 10 activities of interest as our target activities. 'playing with a ball', 'waiting for a car to passed by', 'drinking water', 'feeding', 'turning dog's head to the left', 'turning dog's head to the right', 'petting', 'shaking dog's body by himself', 'sniffing', and 'walking' are the activities of importance we chose to recognize. The videos are in 320*240 image resolution, 48 frames per second. ",http://robotics.ait.kyushu-u.ac.jp/~yumi/db/first_dog.html,http://robotics.ait.kyushu-u.ac.jp/~yumi/db/images/set2.png,Person Animal Activity Recognition from Egocentric Videos,"Takamine, Y. I. A., Kurazume, R., & Ryoo, M. S. (2014). First-Person Animal Activity Recognition from Egocentric Videos. ICPR.[Jain, van Gemert, and Snoek 2014] Jain, M.",ICPR,Kyushu University;California Institute of Technology,
22,FCVID: Fudan-Columbia Video Dataset ,2015,"Yu-Gang Jiang, Zuxuan Wu, Jun Wang, Xiangyang Xue, Shih-Fu Chang",classification,action;event;object;scenes,,video,category labels,91223,239,"In this project, we construct and release a new dataset called Fudan-Columbia Video Dataset (FCVID), containing 91,223 Web videos annotated manually according to 239 categories. ",http://bigvid.fudan.edu.cn/FCVID/,,Exploiting feature and class relationships in video categorization with regularized deep neural networks,"Jiang, Y. G., Wu, Z., Wang, J., Xue, X., & Chang, S. F. (2018). Exploiting feature and class relationships in video categorization with regularized deep neural networks. IEEE transactions on pattern analysis and machine intelligence, 40(2), 352-364.",,"Fudan University;Institute of Data Science and Technology, Columbia University",
23,G3D ,2016,"V. Bloom, V. Argyriou and D. Makris",classification,action,gaming,images;depth images,category labels;skeleton annotations,,20,"G3D dataset contains a range of gaming actions captured with Microsoft Kinect. The Kinect enabled us to record synchronised video, depth and skeleton data. The dataset contains 10 subjects performing 20 gaming actions: punch right, punch left, kick right, kick left, defend, golf swing, tennis swing forehand, tennis swing backhand, tennis serve, throw bowling ball, aim and fire gun, walk, run, jump, climb, crouch, steer a car, wave, flap and clap.",http://dipersec.king.ac.uk/G3D/G3Di.html,http://dipersec.king.ac.uk/G3D/images/Boxing4_7&8.gif,Hierarchical transfer learning for online recognition of compound actions,"Bloom, V., Argyriou, V., & Makris, D. (2016). Hierarchical transfer learning for online recognition of compound actions. Computer Vision and Image Understanding, 144, 62-72.",,Kingston University,
24,G3Di ,2017,"V. Bloom, V. Argyriou and D. Makris",classification,action,gaming,images;depth images,category labels;skeleton annotations,,,"G3Di is a realistic and challenging human interaction dataset for multiplayer gaming, containing synchronised colour, depth and skeleton data. The dataset was captured using a novel gamesourcing approach where the users were recorded whilst playing computer games.

This dataset contains 12 people split into 6 pairs. Each pair interacted through a gaming interface showcasing six sports: boxing, volleyball, football, table tennis, sprint and hurdles. The interactions can be collaborative or competitive depending on the specific sport and game mode. In this dataset volleyball was played collaboratively and the other sports in competitive mode. In most sports the interactions were explicit and can be decomposed by an action and counter action but in the sprint and hurdles the interactions were implicit, as the players competed with each other for the fastest time.

",http://dipersec.king.ac.uk/G3D/G3Di.html,http://dipersec.king.ac.uk/G3D/images/Boxing4_7&8.gif,Hierarchical transfer learning for online recognition of compound actions,"Bloom, V., Argyriou, V., & Makris, D. (2016). Hierarchical transfer learning for online recognition of compound actions. Computer Vision and Image Understanding, 144, 62-72.",,Kingston University,
25,Georgia Tech Egocentric Activities - Gaze(+),2012,"Alireza Fathi, Yin Li, James M. Rehg",classification,action;gaze,,video;audio,temporal category labels;gaze fixation annotations,17,,We have collected these datasets using recently commercially available eye-tracking systems that record a high-quality video of where people look at and at the same time record their gaze location in every frame of the video.,http://ai.stanford.edu/~alireza/GTEA_Gaze_Website/GTEA_Gaze+.html,https://youtu.be/NSKLaRXorf4,,"Fathi, A., Li, Y., & Rehg, J. M. (2012, October). Learning to recognize daily actions using gaze. In European Conference on Computer Vision (pp. 314-327). Springer, Berlin, Heidelberg.",ECCV,Georgia Institute of Technology,
26,HMDB: A Large Human Motion Database,2011,"Kuehne, H., Jhuang, H., Garrote, E., Poggio, T., & Serre, T. ",classification,action,,video,category labels,7000,51,"Here we introduce HMDB collected from various sources, mostly from movies, and a small proportion from public databases such as the Prelinger archive, YouTube and Google videos. The dataset contains 6849 clips divided into 51 action categories, each containing a minimum of 101 clips.",http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/,http://serre-lab.clps.brown.edu/wp-content/uploads/2012/08/HMDB_snapshot1-300x225.png,HMDB: A Large Video Database for Human Motion Recognition,"H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. HMDB: A Large Video Database for Human Motion Recognition. ICCV, 2011",ICCV,Brown University,
27,Hollywood 3D dataset,2013,"Hadfield, S. and Bowden, R",classification;recognition,action,movies,video,category labels,650,14,"This paper presents a new dataset, for benchmarking action recognition algorithms in natural environments, while making use of 3D information. The dataset contains around 650 video clips, across 14 classes",http://cvssp.org/Hollywood3D/,,Hollywood 3D: Recognizing actions in 3D natural scenes,"Hadfield, S., & Bowden, R. (2013, June). Hollywood 3D: Recognizing actions in 3D natural scenes. In Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on(pp. 3398-3405). IEEE.",CVPR,University of Surrey,
28,Hollywood Extended ,2014,"Bojanowski, P., Lajugie, R., Bach, F., Laptev, I., Ponce, J., Schmid, C., & Sivic, J.",classification;pose estimation,action,movies,video,category labels,937,16,Dataset of 937 video clips with a total of 787720 frames containing sequences of 16 different actions from 69 Hollywood movies.,http://www.di.ens.fr/willow/research/actionordering/,http://www.di.ens.fr/willow/research/actionordering/images/teaser.png,Weakly supervised action labeling in videos under ordering constraints,"Bojanowski, P., Lajugie, R., Bach, F., Laptev, I., Ponce, J., Schmid, C., & Sivic, J. (2014, September). Weakly supervised action labeling in videos under ordering constraints. In European Conference on Computer Vision (pp. 628-643). Springer, Cham.",EECV,INRIA;Ecole Normale Superieure,
29,JHMDB: Joint-annotated Human Motion Data Base,2013,H. Jhuang and J. Gall and S. Zuffi and C. Schmid and M. J. Black,classification;pose estimation,action,,video,category labels;joint positions/pose annotations;masks,31838,21,"The HMDB51 database contains more than 5,100
clips of 51 different human actions collected from movies
or the Internet. Annotating this entire dataset is impractical
so J-HMDB is a subset with fewer categories. We
excluded categories that contain mainly facial expressions
like smiling, interactions with others such as shaking hands,
and actions that can only be done in a specific way such as
a cartwheel. The result contains 21 categories involving a
single person in action: brush hair, catch, clap, climb stairs,
golf, jump, kick ball, pick, pour, pull-up, push, run, shoot
ball, shoot bow, shoot gun, sit, stand, swing baseball, throw,
walk, wave. In summary, there are
31,838 annotated frames in total.",http://jhmdb.is.tue.mpg.de/,https://youtu.be/BoQlEo8tPy0,Towards understanding action recognition,"Jhuang, H., Gall, J., Zuffi, S., Schmid, C., & Black, M. J. (2013, December). Towards understanding action recognition. In Computer Vision (ICCV), 2013 IEEE International Conference on (pp. 3192-3199). IEEE.",ICCV,MPI for Intelligent Systems;University of Bonn;Brown University;INRIA,
30,K3Da - Kinect 3D Active dataset,2015,"Leightley, D., Yap, M. H., Coulson, J., Barnouin, Y., & McPhee, J. S.",recognition,action,,depth images;skeleton images,joint positions/pose annotations,225000,,"K3Da (Kinect 3D active) is a realistic clinically relevant human action dataset containing skeleton, depth data and associated participant information. With associated marker indicating noisy/outlier frames.",https://filestore.leightley.com/k3da/,https://filestore.leightley.com/k3da/img/kinectDatasetVis.png,Benchmarking human motion analysis using kinect one: An open source dataset,"Leightley, D., Yap, M. H., Coulson, J., Barnouin, Y., & McPhee, J. S. (2015, December). Benchmarking human motion analysis using kinect one: An open source dataset. In Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2015 Asia-Pacific (pp. 1-7). IEEE.",APSIPA 2015,Manchester Metropolitan University,
31,Kinetics Human Action Video Dataset ,2017,"Will Kay, Joao Carreira,  Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, Andrew Zisserman",classification,action,,video,category labels,300000,400,"The dataset consists of approximately 300,000 video clips, and covers 400 human action classes with at least 400 video clips for each action class. Each clip lasts around 10s and is labeled with a single class. All of the clips have been through multiple rounds of human annotation, and each is taken from a unique YouTube video. The actions cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands and hugging.",https://deepmind.com/research/open-source/open-source-datasets/kinetics/,https://youtu.be/0wR5jVB-WPk,The kinetics human action video dataset,"Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., ... & Suleyman, M. (2017). The kinetics human action video dataset. arXiv preprint arXiv:1705.06950.",,Google Deep Mind,
32,KIT Robo-Kitchen Activity Data Set ,2011,"Lukas Rybok, Simon Friedberger, Uwe D. Hanebeck, and Rainer Stiefelhagen",classification,action,,video,category labels;textual descriptions,540,12,"This Robo-kitchen data set consists of 14 typical kitchen activities recorded in two different stereo-camera setups, and each performed by 17 subjects.",,,,,,Karlsruhe Institute of Technology,
33,LIRIS Human Activities Dataset ,2014,"C Wolf, J. Mille, E. Lombardi, O. Celiktutan, M. Jiu, E. Dogan, G. Eren, M. Baccouche, E. Dellandrea, C.-E. Bichot, C. Garcia, B. Sankur",classification;detection,action,,RGB-D video;images;grayscale images;depth images,temporal category labels;bounding boxes,,828,"The LIRIS human activities dataset contains (gray/rgb/depth) videos showing people performing various activities taken from daily life (discussing, telphone calls, giving an item etc.). The dataset is fully annotated, where the annotation not only contains information on the action class but also its spatial and temporal positions in the video. It was originally shot for the ICPR-HARL 2012 competition.",http://liris.cnrs.fr/voir/activities-dataset/,http://liris.cnrs.fr/voir/activities-dataset/classes/di1.jpg,Evaluation of video activity localizations integrating quality and quantity measurements,,,,
34,MuHAVi ,2017,"Singh, Sanchit, Velastin, Sergio A, Murtaza, Fiza, Yousaf, Muhammad Haroon and Ragheb, Hossein",segmentation;recognition,action,,video frames,temporal category labels;silhouette annotations,,17,"We have collected a large body of human action video (MuHAVi) data using 8 cameras. There are 17 action classes as listed in Table 2 performed by 14 actors. We initially processed videos corresponding to 7 actors in order to split the actions and provide the JPG image frames. These include included some image frames before and after the actual action, for the purpose of background subtraction, tracking, etc. The longest pre-action frames correspond to the actor called Person1. Note that what we provide is therefore temporally pre-segmented actions as this was typical when the dataset was first released. We now provide long unsegmented sequences for people to work on temporal segmentation.",http://velastin.dynu.com/MuHAVi-MAS/,http://velastin.dynu.com/MuHAVi-MAS/images/Walk-C1-A1-V1--.jpg,Multi-view human action recognition using 2D motion templates based on MHIs and their HOG description,"Murtaza, F., Yousaf, M. H., & Velastin, S. A. (2016). Multi-view human action recognition using 2D motion templates based on MHIs and their HOG description. Iet Computer Vision, 10(7), 758-767.",,Kingston University;University of Santiago de Chile;Universidad Carlos III de Madrid,
35,Multi-modal action detection (MAD) Dataset ,2016,"Dong Huang, Yi Wang, Shitong Yao and F. De la Torre",classification;detection,action,,RGB video;depth images;skeleton images,temporal category labels,,35,"The MAD database was recorded using a Microsoft Kinect sensor in an indoor environment.  The database contains i) The multimodal activity of 20 subjects recoded with the Microsoft Kinect camera. The modalities include: RGB video (240*320), 3D depth (240*320), and skeleton (3D coordinates of 20 joints per frame); ii) The activity of 20 subjects performing 35 sequential actions (see actionlist). Each subject repeats the set of 35 actions twice. Each video is about (4000-7000 frames); iii) Ground truth labels: start and end of the action has been labeled, suitable for detection (also for classification).",http://humansensing.cs.cmu.edu/mad/index.html,http://humansensing.cs.cmu.edu/mad/images/don_page_graph_rgb.png,Sequential max-margin event detectors. In European conference on computer vision,"Huang, D., Yao, S., Wang, Y., & De La Torre, F. (2014, September). Sequential max-margin event detectors. In European conference on computer vision (pp. 410-424). Springer, Cham.",ECCV,Carnegie Mellon University,
36,RGB-D activity dataset ,2015,"Chenxia Wu, Jiemi Zhang, Bart Selman, Silvio Savarese, Ashutosh Saxena. ",segmentation;recognition,action,,video;depth images;skeleton images,temporal category labels,,21,"We collect a new challenging RGB-D activity dataset recorded by the Kinect v2 camera. Each video in the dataset contains 2-7 actions interacted with different objects. The Kinect v2 has higher resolution of RGB-D frames (RGB: 1920 × 1080, depth: 512 × 424) and improved body tracking of human skeletons (25 body joints). We record 458 videos with a total length of about 230 minutes. We ask 7 subjects to perform human daily activities in 8 offices and 5 kitchens with complex backgrounds. And in each environment the activities are recorded in different views. It composed of fully annotated 21 types of actions (10 in the office, 11 in the kitchen) interacted with 23 types of objects.",http://watchnpatch.cs.cornell.edu/image/data_sample.png,http://watchnpatch.cs.cornell.edu/image/cvpr15_1.png,Watch-n-patch: Unsupervised understanding of actions and relations,"Wu, C., Zhang, J., Savarese, S., & Saxena, A. (2015, June). Watch-n-patch: Unsupervised understanding of actions and relations. In Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on (pp. 4362-4370). IEEE.",CVPR,Cornell University;Stanford University,
37,SBU Kinect Interaction Dataset ,2012,"Kiwon Yun, Jean Honorio, Debaleena Chattopadhyay, Tamara L. Berg, Dimitris Samaras",classification;segmentation,action,,images;depth images;skeleton images,category labels;joint positions/pose annotations;depth positions;skeleton annotations,300,8,"A complex human activity dataset depicting two person interactions, including synchronized video, depth and motion capture data.",http://www3.cs.stonybrook.edu/~kyun/research/kinect_interaction/index.html,http://www3.cs.stonybrook.edu/~kyun/research/kinect_interaction/images/kinect_actions.jpg,Two-person interaction detection using body-pose features and multiple instance learning,"Yun, K., Honorio, J., Chattopadhyay, D., Berg, T. L., & Samaras, D. (2012, June). Two-person interaction detection using body-pose features and multiple instance learning. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2012 IEEE Computer Society Conference on (pp. 28-35). IEEE.",CVPR,Stony Brook University,
38,SDHA Semantic Description of Human Activities 2010 contest - Human Interactions ,2010,"Ryoo, M. S., Chen, C. C., Aggarwal, J. K., & Roy-Chowdhury, A. ",classification;segmentation;localization,action,,video,temporal category labels,,6,"The UT-Interaction dataset contains videos of continuous executions of 6 classes of human-human interactions: shake-hands, point, hug, push, kick and punch. Ground truth labels for these interactions are provided, including time intervals and bounding boxes. There is a total of 20 video sequences whose lengths are around 1 minute. Each video contains at least one execution per interaction, providing us 8 executions of human activities per video on average. ",http://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html,http://cvrc.ece.utexas.edu/SDHA2010/bmps/challenge1/hand_shaking.jpeg,An overview of contest on semantic description of human activities (SDHA) 2010,"Ryoo, M. S., Chen, C. C., Aggarwal, J. K., & Roy-Chowdhury, A. (2010). An overview of contest on semantic description of human activities (SDHA) 2010. In Recognizing Patterns in Signals, Speech, Images and Videos (pp. 270-285). Springer, Berlin, Heidelberg.",ICCV;ICPR,"University of Texas at Austin;ETRI;University of California, Riverside",
39,SDHA Semantic Description of Human Activities 2010 contest - aerial views ,2010,"Ryoo, M. S., Chen, C. C., Aggarwal, J. K., & Roy-Chowdhury, A. ",classification;localization,action,,video,category labels;bounding boxes,108,9,"In this ""Aerial View Activity Classification Challenge"", the participants will classify human actions in low-resolution videos. The challenge aims to motivate researchers to explore techniques that may address the issues with recognizing human actions in low-resolution videos. This is usually the scenario when videos are filmed from a distant view (e.g. aerial images), and is the common setting for many surveillance and military applications. We provide videos of a single person performing various actions taken from the top of the University of Texas at Austin's main tower. The average height of human figures in this dataset is about 20 pixels. In addition to the resolution issue, there may be challenges from shadows and blurry visual cues. The contest participants are expected to classify videos from a total of 9 categories of human actions: {1: pointing, 2: standing, 3: digging, 4: walking, 5: carrying, 6: running, 7: wave1, 8: wave2, 9: jumping}. ",http://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html,http://cvrc.ece.utexas.edu/SDHA2010/bmps/challenge1/hand_shaking.jpeg,An overview of contest on semantic description of human activities (SDHA) 2010,"Ryoo, M. S., Chen, C. C., Aggarwal, J. K., & Roy-Chowdhury, A. (2010). An overview of contest on semantic description of human activities (SDHA) 2010. In Recognizing Patterns in Signals, Speech, Images and Videos (pp. 270-285). Springer, Berlin, Heidelberg.",,"University of Texas at Austin;ETRI;University of California, Riverside",
40,Sports Videos in the Wild (SVW),2015,"Seyed Morteza Safdarnejad, Xiaoming Liu, Lalita Udpa, Brooks Andrus, John Wood, Dean Craven",classification;recognition;detection;segmentation,action,sports,video,temporal category labels;bounding boxes,4200,74,"SVW is comprised of 4200 videos captured solely with smartphones by users of Coach’s Eye smartphone app, a leading app for sports training developed by TechSmith corporation. SVW includes 30 categories of sports and 44 different actions.  Due to imperfect practice of amateur players and unprofessional capturing by amateur users, SVW is very challenging for automated analysis. Potential applications of SVW include: genre categorization, action recognition, action detection, and spatio-temporal alignment.",http://cvlab.cse.msu.edu/project-svw.html,https://youtu.be/kOgxFx1LVoU,Sports videos in the wild (SVW): A video dataset for sports analysis,"Safdarnejad, S. M., Liu, X., Udpa, L., Andrus, B., Wood, J., & Craven, D. (2015, May). Sports videos in the wild (SVW): A video dataset for sports analysis. In Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on (Vol. 1, pp. 1-7). IEEE.",FG,Michigan State University;TechSmith Corporation,
41,UCLA Human-Human-Object Interaction (HHOI) Dataset  ,2016,"Tianmin Shu, M. S. Ryoo and Song-Chun Zhu",recognition,action,,RGB-D video;images;depth images;skeleton images,skeleton annotations;depth positions,118,5,Human interactions in RGB-D videos.,http://www.stat.ucla.edu/~tianmin.shu/SocialAffordance/index.html,http://www.stat.ucla.edu/~tianmin.shu/SocialAffordance/Intro3.png, Learning social affordance for human-robot interaction,"Shu, T., Ryoo, M. S., & Zhu, S. C. (2016). Learning social affordance for human-robot interaction. arXiv preprint arXiv:1604.03692.",IJCAI,"University of California, Los Angeles;Indiana University",
42,UCR Videoweb Activities Dataset,2010,"G. Denina, B. Bhanu, H. Nguyen, C. Ding, A. Kamal, C. Ravishankar, A. Roy-Chowdhury, A. Ivers, and B. Varda",classification;recognition;tracking,person,,video;images,temporal category labels,51,51,The VideoWeb Activities Dataset described in this chapter aims to fill this need by providing a diverse set of annotated multi-camera footage where the data are collected in a realistic environment and the subjects are real actors who are mimicking everyday NVC activities. The VideoWeb dataset is a collection of 2.5 hours of 51 hand-annotated scenes. Activities are performed by at least four groups of actors where each consists of four persons. The data are recorded simultaneously by four to eight cameras at full 640 × 480 resolution and approximately 30 frames/second.,http://vcg.engr.ucr.edu/datasets/,,VideoWeb Dataset for Multi-camera Activities and Non-verbal Communication,"G. Denina, B. Bhanu, H. Nguyen, C. Ding, A. Kamal, C. Ravishankar, A. Roy-Chowdhury, A. Ivers, and B. Varda, ""VideoWeb Dataset for Multi-camera Activities and Non-verbal Communication"", in Distributed Video Sensor Networks (Eds. B. Bhanu, C. Ravishankar, A. Roy-Chowdhury, H. Aghajan, D. Terzopoulos), Springer 2010. ",,"University of California, Riverside",
43,UCR Tour20 Video Summarization Dataset,2017,"R. Panda, N.C. Mithun, A. Roy-Chowdhury",description/captioning,video,,video,textual descriptions,140,,"Tour20 is a video summarization dataset that is designed primarily for multi-video summarization. However, it can also be used for evaluating single-video summarization in a repeatable and efficient way. It contains 140 videos of total 6 hour 46 minutes duration that are downloaded from YouTube with creative commons license, CC-By 3.0. The dataset consists of three human created ground truth summaries for each of the videos as well as a diverse set of summary to describe the video collection of a tourist place. We also provide the shot segmentation files that indicate the shot boundary transitions of each video.",http://vcg.engr.ucr.edu/datasets/,,Diversity-aware Multi-Video Summarization,"R. Panda, N.C. Mithun, A. Roy-Chowdhury, ""Diversity-aware Multi-Video Summarization"" in IEEETransactions on Image Processing, 2017",,"University of California, Riverside",
44,UCR DivNet Image Dataset,2016,"N. C. Mithun, R. Panda and A. K. Roy-Chowdhury,",classification,images,,images,category labels,550000,550,"The DivNet dataset contains images for around 550 object and scene categories, averaging around 1K images per category. The categories are mainly chosen from ILSVRC2016 object detection and scene classification challenge. The images for each category were originally collected from Google, Bing and Flickr. ",http://vcg.engr.ucr.edu/datasets/,,Generating Diverse Image Datasets with Limited Labeling,"N. C. Mithun, R. Panda and A. K. Roy-Chowdhury, ""Generating Diverse Image Datasets with Limited Labeling"" in ACM MM, Amsterdam, October, 2016.",MM,"University of California, Riverside",
45,UTD-MHAD ,2015,"Chen Chen, Roozbeh Jafari, and Nasser Kehtarnavaz",classification;recognition,action,,RGB-D video;sensor recordings;skeleton images;depth images,category labels,861,27,"The UTD-MHAD dataset was collected using a Microsoft Kinect sensor and a wearable inertial sensor in an indoor environment. The dataset contains 27 actions performed by 8 subjects (4 females and 4 males). Each subject repeated each action 4 times. After removing three corrupted sequences, the dataset includes 861 data sequences. Four data modalities of RGB videos, depth videos, skeleton joint positions, and the inertial sensor signals were recorded in three channels or threads. ",http://www.utdallas.edu/~kehtar/UTD-MHAD.html,http://www.utdallas.edu/~kehtar/UTD-MAD/swipe_left.png,,,ICIP,University of Texas at Dallas,
46,UTKinect dataset ,2012,"Xia, L. and Chen, C.C. and Aggarwal, JK",recognition;classification,action,,RBG images;depth images,category labels,200,10,"The videos was captured using a single stationary Kinect with Kinect for Windows SDK Beta Version. There are 10 action types: walk, sit down, stand up, pick up, carry, throw, push, pull, wave hands, clap hands. There are 10 subjects, Each subject performs each actions twice. Three channels were recorded: RGB, depth and skeleton joint locations.",http://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html,http://cvrc.ece.utexas.edu/KinectDatasets/10actions.png,View Invariant Human Action Recognition Using Histograms of 3D Joints,"Xia, L., Chen, C. C., & Aggarwal, J. K. (2012, June). View invariant human action recognition using histograms of 3d joints. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2012 IEEE Computer Society Conference on (pp. 20-27). IEEE.",CVPR,The University of Texas at Austin,
47,WorkoutSU-10 dataset ,2013,"Farhood Negin, Firat Ozdemir, Ceyhun Burak Akgul, Kamer Ali Yuksel",recognition,action,,video,skeleton annotations;joint positions/pose annotations,1500,10,The WorkoutSU-10 exercise dataset comprises a collection of sequences of human body movements represented by 3D positions of skeletal joints. ,http://vpa.sabanciuniv.edu.tr/phpBB2/vpa_views.php?s=31&serial=36,http://vpa.sabanciuniv.edu.tr/databases/db-s36_img200.jpg,A Decision Forest Based Feature Selection Framework for Action Recognition from RGB-Depth Cameras,"Negin, F., Özdemir, F., Akgül, C. B., Yüksel, K. A., & Erçil, A. (2013, June). A decision forest based feature selection framework for action recognition from rgb-depth cameras. In International Conference Image Analysis and Recognition (pp. 648-657). Springer, Berlin, Heidelberg.",ICIAR,Sabanc University;Vistek ISRA;Bagazici Univeristy,
48,Wrist-mounted camera video dataset ,2016,"Katsunori Ohnishi, Atsushi Kanehira, Asako Kanezaki, Tatsuya Harada,",classification,action,,video frames,category labels,690000,23,"We present a novel dataset and a novel algorithm for recognizing activities of daily living (ADL) from a first-person, wrist-mounted camera. ",http://www.mi.t.u-tokyo.ac.jp/static/projects/miladl/,http://www.mi.t.u-tokyo.ac.jp/static/projects/miladl/icon.jpg,Recognizing Activities of Daily Living with a Wrist-mounted Camera,"Ohnishi, K., Kanehira, A., Kanezaki, A., & Harada, T. (2016). Recognizing activities of daily living with a wrist-mounted camera. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 3103-3111).",CVPR,University of Tokyo,
49,YouCook,2013,"P. Das, C. Xu, R. F. Doell, and J. J. Corso",description/captioning,action,food,video;video frames,textual descriptions,88,,This data set was prepared from 88 open-source YouTube cooking videos. The YouCook dataset contains videos of people cooking various recipes. The videos were downloaded from YouTube and are all in the third-person viewpoint; they represent a significantly more challenging visual problem than existing cooking and kitchen datasets (the background kitchen/scene is different for many and most videos have dynamic camera changes). ,https://www.cse.buffalo.edu/~jcorso/r/youcook/,https://www.cse.buffalo.edu//~jcorso/r/youcook/files/youcook_example_lores.png,A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching,"Das, P., Xu, C., Doell, R. F., & Corso, J. J. (2013, June). A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching. In Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on (pp. 2634-2641). IEEE.",CVPR,SUNY at Buffalo,
50,YouTube-8M,2016,"Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadarajan, Sudheendra Vijayanarasimhan",classification,video,,audio;video,category labels,7000000,4716,"YouTube-8M is a large-scale labeled video dataset that consists of millions of YouTube video IDs and associated labels from a diverse vocabulary of 4700+ visual entities. It comes with precomputed state-of-the-art audio-visual features from billions of frames and audio segments, designed to fit on a single hard disk. ",https://research.google.com/youtube8m/,,YouTube-8M: A Large-Scale Video Classification Benchmark,"Abu-El-Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici, G., Varadarajan, B., & Vijayanarasimhan, S. (2016). Youtube-8m: A large-scale video classification benchmark. arXiv preprint arXiv:1609.08675.",,Google Research,
51,CMUSRD: Surveillance Research Dataset,2014,"Koosuke Hattori, Hironori Hattori, Yuji Ono, Katsuaki Nishino, Masaya Itoh, Vishnu Naresh Boddeti and Takeo Kanade",tracking;detection,surveillance;person,indoor;social events,video,identity annotations;bounding boxes,,,"We start a project to collect a comprehensive new dataset for indoor surveillance scenario. The collection site is a CMU building where multiple cameras are installed. More than several tens of people are used as subjects, and the obtained data will be ground-truthed in terms of people’s identity and position in the image.",http://www.consortium.ri.cmu.edu/projSRD.php#site,http://www.consortium.ri.cmu.edu/data/SRD/allcamimage.jpg,Image Dataset for Researches about Surveillance Camera - CMU_SRD (Surveillance Research Dataset) ,"Hattori, K., Hattori, H., Ono, Y., Nishino, K., Itoh, M., Boddeti, V. N., & Kanade, T. Image Dataset for Researches about Surveillance Camera-?CMU_SRD (Surveillance Research Dataset)-?.",,Carnegie Mellon University,
52,DukeMTMC: Duke Multi-Target Multi-Camera tracking dataset,2016,"Ergys Ristani, Francesco Solera, Roger S. Zou, Rita Cucchiara, Carlo Tomasi.",tracking;detection,surveillance;pedestrian,,video frames,identity annotations;bounding boxes,2000000,,"DukeMTMC aims to accelerate advances in multi-target multi-camera tracking. It provides a tracking system that works within and across cameras, a new large scale HD video data set recorded by 8 synchronized cameras with more than 7,000 single camera trajectories and over 2,000 unique identities, and a new performance evaluation method that measures how often a system is correct about who is where.",http://vision.cs.duke.edu/DukeMTMC/,http://vision.cs.duke.edu/DukeMTMC/img/dataset.jpg,"Performance Measures and a Data Set for Multi-Target, Multi-Camera Tracking","Ristani, E., Solera, F., Zou, R., Cucchiara, R., & Tomasi, C. (2016, October). Performance measures and a data set for multi-target, multi-camera tracking. In European Conference on Computer Vision (pp. 17-35). Springer, Cham.",ECCV,Duke University;University of Modena and Reggio Emilia,
53,Parking-Lot Dataset ,2014,"Bo Li, Tianfu Wu, Song-Chun Zhu",detection,vehicle,,images,bounding boxes,,,Parking-Lot dataset is a car dataset which focus on moderate and heavily occlusions on cars in the parking lot,http://www.stat.ucla.edu/~boli/projects/context_occlusion/context_occlusion.html,,,,ECCV,"University of California, Los Angeles;Beijing Institute of Technology",
54,Princeton Tracking Benchmark,2013,Shuran Song and Jianxiong Xiao,tracking,surveillance;traffic,,images;depth images,bounding boxes,100,," In this paper, we construct a unified benchmark dataset of 100 RGBD videos with high diversity, propose different kinds of RGBD tracking algorithms using 2D or 3D model, and present a quantitative comparison of various algorithms with RGB or RGBD input.",http://tracking.cs.princeton.edu/dataset.html,http://tracking.cs.princeton.edu/imgs/datasetimg.png,Tracking Revisited using RGBD Camera: Unified Benchmark and Baselines,"Song, S., & Xiao, J. (2013, December). Tracking revisited using RGBD camera: Unified benchmark and baselines. In Computer Vision (ICCV), 2013 IEEE International Conference on (pp. 233-240). IEEE.",ICCV,Princeton University,
55,Queen Mary Multi-Camera Distributed Traffic Scenes Dataset (QMDTS),2017,"Xun Xu, Timothy M.Hospedales, and Shaogang Gong",description/captioning,surveillance;traffic,,surveillance video,semantic annotations,27,,This dataset consists of surveillance video footage from 27 distributed and disjoint camera views of busy traffic scenes in urban environments. This dataset includes two existing traffic scene datasets: QMUL Junction Datasetand QMUL Roundabout Dataset and a brand new 25 traffic scenes collected from urban environment.,https://alex-xun-xu.github.io/ProjectPage/TCSVT_15/index.html,https://alex-xun-xu.github.io/ProjectPage/TCSVT_15/img/DatasetSampleFrames_Intro.png,Discovery of Shared Semantic Spaces for Multi-Scene Video Query and Summarization,"Xu, X., Hospedales, T. M., & Gong, S. (2017). Discovery of shared semantic spaces for multiscene video query and summarization. IEEE Transactions on Circuits and Systems for Video Technology, 27(6), 1353-1367.",,Queen Mary University of London,
56,SALSA: Synergetic sociAL Scene Analysis ,2015,"X. Alameda-Pineda, J. Staiano, R. Subramanian, L. Batrinca, E. Ricci, B. Lepri, O. Lanz and N. Sebe",tracking;pose estimation,surveillance,indoor;social events,surveillance video,bounding boxes;joint positions/pose annotations,,,"SALSA: Synergetic sociAL Scene Analysis dataset contains uninterrupted recordings of an indoor social event involving 18 subjects over 60 minutes. It serves as a rich and extensive repository for the behavioral analysis and social signal processing communities. In addition to the raw multimodal data, SALSA also contains position, pose and F-formation annotations over the entire event duration for evaluation purposes, as well as information regarding participants’ personality traits.",http://tev.fbk.eu/salsa,http://tev.fbk.eu/sites/tev.fbk.eu/files/salsa-web5.png,SALSA: A Novel Dataset for Multimodal Group Behavior Analysis,"Alameda-Pineda, X., Staiano, J., Subramanian, R., Batrinca, L., Ricci, E., Lepri, B., ... & Sebe, N. (2016). Salsa: A novel dataset for multimodal group behavior analysis. IEEE transactions on pattern analysis and machine intelligence, 38(8), 1707-1720.",,Fondazione Bruno Kessler,
57,SBM-RGBD dataset,2017,"M. Camplani, L. Maddalena, G. Moyà Alcover, A. Petrosino, L. Salgado",detection,surveillance;object;scenes,,surveillance video;video frames;RGB-D video,pixel-wise segmentation;depth positions,15000,,"The SBM-RGBD dataset provides a diverse set of groundtruthed synchronized color and depth sequences acquired by the Microsoft Kinect. The dataset consists of 33 videos (~15000 frames) representative of typical indoor visual data captured in video surveillance and smart environment scenarios, selected to cover a wide range of scene background modeling challenges for moving object detection. ",http://rgbd2017.na.icar.cnr.it/SBM-RGBDdataset.html,,A Benchmarking Framework for Background Subtraction in RGBD videos,"Camplani, M., Maddalena, L., Alcover, G. M., Petrosino, A., & Salgado, L. (2017, September). A benchmarking framework for background subtraction in RGBD videos. In International Conference on Image Analysis and Processing (pp. 219-229). Springer, Cham.",ICIAP,"University of Bristol;National Research Council;Universitat de les Illes Balears, University of Naples Parthenope;Universidad Politécnica de Madrid & Universidad Autónoma de Madrid",
58,Shinpuhkan 2014 ,2014,"Y.Kawanishi, Y.Wu, M.Mukunoki and M.Minoh",identification;recognition,surveillance;person,,surveillance video;video frames,identity annotations;tracking/movement annotations,22000,24,We present a novel manually annotated dataset which contains multiple tracklets for each person captured by multiple overlapping/non-overlapping cameras in surveillance scenarios.,http://www.mm.media.kyoto-u.ac.jp/en/datasets/shinpuhkan/,http://www.mm.media.kyoto-u.ac.jp/wp-content/uploads/2014/05/persons.jpg,A Multi-Camera Pedestrian Dataset for Tracking People across Multiple Cameras,"Kawanishi, Y., Wu, Y., Mukunoki, M., & Minoh, M. (2014). Shinpuhkan2014: A multi-camera pedestrian dataset for tracking people across multiple cameras. In 20th Korea-Japan Joint Workshop on Frontiers of Computer Vision (Vol. 5, No. 7).",FCV,Kyoto University,
59,Stanford Drone Dataset ,2016,"A. Robicquet, A. Sadeghian, A. Alahi, S. Savarese",detection;localization,surveillance;traffic,outdoor,surveillance video;video frames,bounding boxes;identity annotations,100,6,"The very first large scale dataset (to the best of our knowledge) that collects images and videos of various types of agents (not just pedestrians, but also bicyclists, skateboarders, cars, buses, and golf carts) that navigate in a real world outdoor environment such as a university campus.",http://cvgl.stanford.edu/projects/uav_data/,http://cvgl.stanford.edu/projects/uav_data/bookstore.jpg,Learning Social Etiquette: Human Trajectory Prediction In Crowded Scenes,"A. Robicquet, A. Sadeghian, A. Alahi, S. Savarese, Learning Social Etiquette: Human Trajectory Prediction In Crowded Scenes in European Conference on Computer Vision (ECCV), 2016.",,Stanford University,
60,UCLA Aerial Event Dataset ,2015,Tianmin Shu and Dan Xie and Brandon Rothrock and Sinisa Todorovic and Song-Chun Zhu,recognition,surveillance;person;event;object,,surveillance video;video frames,bounding boxes;category labels,27,12,"A new set of aerial videos using a hex-rotor flying over picnic areas rich with group events. The annotation in our dataset includes individuals, objects, groups, events, human roles and goals (destinations).",http://www.stat.ucla.edu/~tianmin.shu/AerialVideo/AerialVideo.html,http://www.stat.ucla.edu/~tianmin.shu/AerialVideo/pipeline.png,"Joint inference of groups, events and human roles in aerial videos","Shu, T., Xie, D., Rothrock, B., Todorovic, S., & Zhu, S. C. (2015). Joint inference of groups, events and human roles in aerial videos. arXiv preprint arXiv:1505.05957.",CVPR,University of California Los Angeles;California Institute of Technology;Oregon State University,
61,WIDER Attribute Dataset,2016,"Yining Li, Chen Huang, Chen Change Loy, and Xiaoou Tang",recognition,attributes;person,,images,bounding boxes;attribute annotations,13789,14,"WIDER Attribute is a large-scale human attribute dataset. It contains 13789 images belonging to 30 scene categories, and 57524 human bounding boxes each annotated with 14 binary attributes.",http://mmlab.ie.cuhk.edu.hk/projects/WIDERAttribute.html,http://mmlab.ie.cuhk.edu.hk/projects/WIDERAttribute_files/wider_sample.png,Human attribute recognition by deep hierarchical contexts,"Li, Y., Huang, C., Loy, C. C., & Tang, X. (2016, October). Human attribute recognition by deep hierarchical contexts. In European Conference on Computer Vision (pp. 684-700). Springer, Cham.",ECCV,The Chinese University of Hong Kong,
62,Large-scale CelebFaces Attributes (CelebA) Dataset,2015,"Ziwei Liu. Ping Luo, Xiaogang Wang, Xiaoou Tang",recognition;detection;localization,attributes;person,,images,bounding boxes;attribute annotations,200000,,"CelebFaces Attributes Dataset (CelebA) is a large-scale face attributes dataset with more than 200K celebrity images, each with 40 attribute annotations. The images in this dataset cover large pose variations and background clutter. CelebA has large diversities, large quantities, and rich annotations",http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html,http://mmlab.ie.cuhk.edu.hk/projects/celeba/overview.png,From Facial Parts Responses to Face Detection: A Deep Learning Approach,"S. Yang, P. Luo, C. C. Loy, and X. Tang, ""From Facial Parts Responses to Face Detection: A Deep Learning Approach"", in IEEE International Conference on Computer Vision (ICCV), 2015",ICCV,The Chinese University of Hong Kong,
63,DukeMTMC-attribute,2017,"Lin, Y., Zheng, L., Zheng, Z., Wu, Y., & Yang, Y. ",recognition,attributes;person,,images,attribute annotations,1812,,"We annotate 23 attributes for DukeMTMC-reID, which is a subset of the DukeMTMC. The original dataset contains 702 identities for training and 1110 identities for testing. The attributes are annotated in the identity level, thus the file contains 23 x 702 attributes for training and 23 x 1110 for test.",https://github.com/vana77/DukeMTMC-attribute,https://github.com/vana77/DukeMTMC-attribute/blob/master/sample_image.jpg?raw=true, Improving person re-identification by attribute and identity learning,"Lin, Y., Zheng, L., Zheng, Z., Wu, Y., & Yang, Y. (2017). Improving person re-identification by attribute and identity learning. arXiv preprint arXiv:1703.07220.",,Duke University,
64,Database of Human Attributes (HAT),2011,Gaurav Sharma and Frederic Jurie,recognition,attributes;person,,images,attribute annotations,9344,,"Database for learning semantic human attributes. The database contains a wide variety of human images in different poses (standing, sitting, running, turned back etc.), of different ages (baby, teen, young, middle aged, elderly etc.), wearing different clothes (tee-shirt, suits, beachwear, shorts etc.) and accessories (sunglasses, bag etc.) and is, thus, rich in semantic attributes for humans. It also has high variation in scale (only upper body to the full person) and size of the images. ",https://jurie.users.greyc.fr/datasets/hat.html,https://jurie.users.greyc.fr/datasets/pics/attribeg1.jpg, Learning discriminative spatial representation for image classification,"Sharma, G., & Jurie, F. (2011, August). Learning discriminative spatial representation for image classification. In BMVC 2011-British Machine Vision Conference (pp. 1-11). BMVA Press.",BMVC,INRIA Grenoble Rhone-Alpes;University of Caen,
65,LFW-10 dataset,2014,"Ramachandruni N. Sandeep, Yashaswi Verma, C. V. Jawahar",recognition,attributes;person,,images,attribute annotations,2000,,"A dataset of 10,000 pairs of face images with instance-level annotations for 10 attributes.",http://cvit.iiit.ac.in/projects/relativeParts/,http://cvit.iiit.ac.in/projects/relativeParts/1.png,Relative parts: Distinctive parts for learning relative attributes,"Sandeep, R. N., Verma, Y., & Jawahar, C. V. (2014, June). Relative parts: Distinctive parts for learning relative attributes. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on (pp. 3614-3621). IEEE.",CVPR,IIIT Hyderabad,
66,Market-1501,2015,"Zheng, L., Shen, L., Tian, L., Wang, S., Wang, J., & Tian, Q.",detection,pedestrian,,surveillance video;video frames,bounding boxes;identity annotations,1501,,"The Market-1501 dataset is collected in front of a supermarket in Tsinghua University. A total of six cameras are used, including 5 high-resolution cameras, and one low-resolution camera. Overlap exists among different cameras. Overall, this dataset contains 32,668 annotated bounding boxes of 1,501 identities. ",http://www.liangzheng.org/Project/project_reid.html,http://www.liangzheng.org/Project/dataset.jpg,Scalable person re-identification: A benchmark,"Zheng, L., Shen, L., Tian, L., Wang, S., Wang, J., & Tian, Q. (2015). Scalable person re-identification: A benchmark. In Proceedings of the IEEE International Conference on Computer Vision (pp. 1116-1124).",ICCV,Tsinghua University;Microsoft Research;University of Texas at San Antonio,
67,Market-1501_Attribute,2017,"Lin, Y., Zheng, L., Zheng, Z., Wu, Y., & Yang, Y.",recognition,attributes;person,food,images,attribute annotations,1501,,"We annotate 27attributes for Market-1501. The original dataset contains 751 identities for training and 750 identities for testing. The attributes are annotated in the identity level, thus the file contains 27 x 751 attributes for training and 27 x 750 attributesfor test.",https://github.com/vana77/Market-1501_Attribute,https://github.com/vana77/Market-1501_Attribute/raw/master/sample_image.jpg,Improving person re-identification by attribute and identity learning,"Lin, Y., Zheng, L., Zheng, Z., Wu, Y., & Yang, Y. (2017). Improving person re-identification by attribute and identity learning. arXiv preprint arXiv:1703.07220.",,University of Technology Sydney,
68,People In Photo Albums (PIPA),2015,Ning Zhang and Manohar Paluri and Yaniv Taigman and Rob Fergus and Lubomir Bourdev,recognition,person,,images,bounding boxes;identity annotations,60000,,"We explore the task of recognizing peoples' identities in photo albums in an unconstrained setting. To facilitate this, we introduce the new People In Photo Albums (PIPA) dataset, consisting of over 60000 instances of ~2000 individuals collected from public Flickr photo albums. ",https://people.eecs.berkeley.edu/~nzhang/piper.html,https://people.eecs.berkeley.edu/~nzhang/figs/clothes.jpg,Beyond frontal faces: Improving person recognition using multiple cues,"Zhang, N., Paluri, M., Taigman, Y., Fergus, R., & Bourdev, L. (2015). Beyond frontal faces: Improving person recognition using multiple cues. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 4804-4813).",,UC Berkeley;Facebook AI Research,
69,Person Recognition in Personal Photo Collections,2015,"Seong Joon Oh, Rodrigo Benenson, Mario Fritz, Bernt Schiele",recognition,attributes;person,,images,attribute annotations,60000,,Provides long term attribute annotations for People In Photo Albums (PIPA) dataset,https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/people-detection-pose-estimation-and-tracking/person-recognition-in-personal-photo-collections/,https://www.mpi-inf.mpg.de/fileadmin/_processed_/d/e/csm_split_textadded_v2_arxiv_317bd3ca3c.jpg,Person Recognition in Personal Photo Collections,"Oh, S. J., Benenson, R., Fritz, M., & Schiele, B. (2015, December). Person Recognition in Personal Photo Collections. In ICCV (pp. 3862-3870).",ICCV,Max Planck Institute for Informatics,
70,Visual Attributes Dataset,2013,"Carina Silberer, Vittorio Ferrari, Mirella Lapata",recognition;classification,attributes;person,,images,attribute annotations,688000,500,This dataset contains visual attribute annotations for over 500 concrete (animate and inanimate) concepts.,http://homepages.inf.ed.ac.uk/s1151656/resources.html,,Models of semantic representation with visual attributes,"Silberer, C., Ferrari, V., & Lapata, M. (2013). Models of semantic representation with visual attributes. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (Vol. 1, pp. 572-582).",,University of Edinburgh,
71,The Visual Privacy (VISPR) Dataset,2017,"Tribhuvanesh Orekondy, Bernt Schiele, Mario Fritz",recognition;classification,attributes;person,privacy,images,attribute annotations,22100,,Dataset of  personal information in images categorized into 68 image attributes.,https://tribhuvanesh.github.io/vpa/,,Towards a visual privacy advisor: Understanding and predicting privacy risks in images,"Orekondy, T., Schiele, B., & Fritz, M. (2017, October). Towards a visual privacy advisor: Understanding and predicting privacy risks in images. In 2017 IEEE International Conference on Computer Vision (ICCV) (pp. 3706-3715). IEEE.",ICCV,Max Planck Institute for Informatics,
72,AMUSE,2013,Michael Felsberg and Lars Nielsen and Rudolf Mester,environment sensing,vehicle,,images;sensor recordings,n/a,,,"The automotive multi-sensor (AMUSE) dataset consists of inertial and other complementary sensor data combined with monocular, omnidirectional, high frame rate visual data taken in real traffic scenes during multiple test drives.",http://www.cvl.isy.liu.se/en/research/datasets/amuse/,,A multi-sensor traffic scene dataset with omnidirectional video,"Koschorrek, P., Piccini, T., Öberg, P., Felsberg, M., Nielsen, L., & Mester, R. (2013, June). A multi-sensor traffic scene dataset with omnidirectional video. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2013 IEEE Conference on(pp. 727-734). IEEE.",CVPR,Linkoping University;University of Frankfurt,
73,Joint Attention in Autonomous Driving (JAAD) ,2016,"Iuliia Kotseruba, Amir Rasouli, John K. Tsotsos",recognition,action;pedestrian;traffic,,video;video frames,textual descriptions;category labels;bounding boxes,88000,,"JAAD is a new dataset for studying joint attention in the context of autonomous driving. In particular, the focus is on pedestrian and driver behaviors at the point of crossing and factors that influence them. To this end, JAAD dataset provides an annotated collection of short video clips representing scenes typical for everyday urban driving in various weather conditions.",http://data.nvision2.eecs.yorku.ca/JAAD_dataset/,http://data.nvision2.eecs.yorku.ca/JAAD_dataset/images/timeline_v2-01.png,Are They Going to Cross? A Benchmark Dataset and Baseline for Pedestrian Crosswalk Behavior,"Rasouli, A., Kotseruba, I., & Tsotsos, J. K. (2017). Are They Going to Cross? A Benchmark Dataset and Baseline for Pedestrian Crosswalk Behavior. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 206-213).;Kotseruba, I., Rasouli, A., & Tsotsos, J. K. (2016). Joint attention in autonomous driving (JAAD). arXiv preprint arXiv:1609.04741.",ICCV,York University,
74,LISA Vehicle Detection Dataset,2010,Sayanan Sivaraman and Mohan M. Trivedi,recognition;tracking,vehicle,attention,video,bounding boxes,,,"Three color video sequences captured at different times of the day and illumination settings: morning, evening, sunny, cloudy, etc. Different driving environments: highway and urban. Varying traffic conditions: light to dense traffic.",http://cvrr.ucsd.edu/LISA/vehicledetection.html,http://cvrr.ucsd.edu/LISA/alvert.jpg,A general active-learning framework for on-road vehicle recognition and tracking,"Sivaraman, S., & Trivedi, M. M. (2010). A general active-learning framework for on-road vehicle recognition and tracking. IEEE Transactions on Intelligent Transportation Systems, 11(2), 267-276.",,University of California San Diego,
75,Lost and Found Dataset ,2016,"Pinggera, P., Ramos, S., Gehrig, S., Franke, U., Rother, C., & Mester, R.",detection,traffic,,video;video frames,pixel-wise segmentation,,,The Lost And Found Dataset addresses the problem of detecting unexpected small obstacles on the road often caused by lost cargo. The dataset comprises 112 stereo video sequences with 2104 annotated frames (picking roughly every tenth frame from the recorded data).,http://www.6d-vision.com/lostandfounddataset,http://www.6d-vision.com/_/rsrc/1475850957615/lostandfounddataset/label.jpg?height=99&width=200,Lost and found: detecting small road hazards for self-driving vehicles,"Pinggera, P., Ramos, S., Gehrig, S., Franke, U., Rother, C., & Mester, R. (2016, October). Lost and found: detecting small road hazards for self-driving vehicles. In Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on(pp. 1099-1106). IEEE.",,Daimler R&D; TU Dresden; Goethe University ,
76,SYNTHIA,2016,German Ros and Laura Sellart and Joanna Materzynska and David Vazquez and Antonio Lopez,segmentation,city;traffic,driving,images;depth frames,pixel-wise segmentation,400000,13,"The  SYNTHetic collection of Imagery and Annotations, is a dataset that has been generated with the purpose of aiding semantic segmentation and related scene understanding problems in the context of driving scenarios. SYNTHIA consists of a collection of photo-realistic frames rendered from a virtual city and comes with precise pixel-level semantic annotations.",http://synthia-dataset.net/,,The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes,"Ros, G., Sellart, L., Materzynska, J., Vazquez, D., & Lopez, A. M. (2016). The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 3234-3243).",CVPR,Edifici O;Universitat Autonoma de Barcelona;University of Vienna,
77,BRATS,2017,,segmentation,medical,tumor,images,pixel-wise segmentation,243,,"Dataset includes ample multi-institutional routine clinically-acquired pre-operative multimodal MRI scans of glioblastoma (GBM/HGG) and lower grade glioma (LGG), with pathologically confirmed diagnosis and available OS for training, validation and testing.",http://braintumorsegmentation.org/,http://www.med.upenn.edu/sbia/assets/user-content/BRATS_tasks.png,The multimodal brain tumor image segmentation benchmark (BRATS),"Menze, B. H., Jakab, A., Bauer, S., Kalpathy-Cramer, J., Farahani, K., Kirby, J., ... & Lanczi, L. (2015). The multimodal brain tumor image segmentation benchmark (BRATS). IEEE transactions on medical imaging, 34(10), 1993-2024.;Bakas, S., Akbari, H., Sotiras, A., Bilello, M., Rozycki, M., Kirby, J. S., ... & Davatzikos, C. (2017). Advancing The Cancer Genome Atlas glioma MRI collections with expert segmentation labels and radiomic features. Scientific data, 4, 170117.",,"Perelman School of Medicine, University of Pennsylvania",
78,Cholec80,2017,"A.P. Twinanda, S. Shehata, D. Mutter, J. Marescaux, M. de Mathelin, N. Padoy",recognition,medical,surgery,video,temporal category labels,80,,The Cholec80 dataset contains 80 videos of cholecystectomy surgeries performed by 13 surgeons. The videos are captured at 25 fps. The dataset is labeled with the phase (at 25 fps) and tool presence annotations (at 1 fps).,http://camma.u-strasbg.fr/datasets,,Endonet: A deep architecture for recognition tasks on laparoscopic videos,"Twinanda, A. P., Shehata, S., Mutter, D., Marescaux, J., de Mathelin, M., & Padoy, N. (2017). Endonet: A deep architecture for recognition tasks on laparoscopic videos. IEEE transactions on medical imaging, 36(1), 86-97.",,,
79,CRCHistoPhenotypes - Labeled Cell Nuclei Data ,2016,"Korsuk Sirinukunwattana, Shan E Ahmed Raza, Yee-Wah Tsang, David R. J. Snead, Ian A. Cree & Nasir M. Rajpoot",detection,medical,cells,images,bounding boxes;category labels,"29,756 ",4,"This dataset involves 100 H&E stained histology images of colorectal adenocarcinomas. A total of 29,756 nuclei were marked at/around the center for detection purposes. Out of these, there were 22,444 nuclei that also have an associated class label, i.e. epithelial, inflammatory, fibroblast, and miscellaneous. ",https://warwick.ac.uk/fac/sci/dcs/research/tia/data/crchistolabelednucleihe/,,,,,University of Warwick; University Hospitals Coventry and Warwickshire;Qatar University,
80,Cell Tracking Challenge Datasets ,2015,,segmentation;tracking,medical,cells,video;3D video,pixel-wise segmentation,,,"In this challenge we objectively compare and evaluate state-of-the-art whole-cell and nucleus tracking methods using both real (2D and 3D) time-lapse microscopy videos of labeled cells and nuclei, along with computer generated video sequences simulating nuclei moving in realistic environments",http://www.celltrackingchallenge.net/,http://www.celltrackingchallenge.net/Media/combined_01.gif,An objective comparison of cell-tracking algorithms,"Ulman, V., Maška, M., Magnusson, K. E., Ronneberger, O., Haubold, C., Harder, N., ... & Smal, I. (2017). An objective comparison of cell-tracking algorithms. Nature methods, 14(12), 1141.",,,
81,FIRE Fundus Image Registration Dataset ,2017,"C. Hernandez-Matas, X. Zabulis, A. Triantafyllou, P. Anyfanti, S. Douma",imaging/image processing,medical,retina;image registration,images,localization line/shape coordinates;masks,129,,"The dataset consists of 129 retinal images forming 134 image pairs. These image pairs are split into 3 different categories depending on their characteristics. The images were acquired with a Nidek AFC-210 fundus camera, which acquires images with a resolution of 2912x2912 pixels and a FOV of 45° both in the x and y dimensions. Images were acquired at the Papageorgiou Hospital, Aristotle University of Thessaloniki, Thessaloniki from 39 patients.",http://www.ics.forth.gr/cvrl/fire/,http://www.ics.forth.gr/cvrl/fire/img/A5_2_kp.jpg, FIRE: fundus image registration dataset,"Hernandez-Matas, C., Zabulis, X., Triantafyllou, A., Anyfanti, P., Douma, S., & Argyros, A. A. (2017). FIRE: fundus image registration dataset. Journal for Modeling in Ophthalmology, 1(4), 16-28.",,Hellas (FORTH);University of Crete;Papageorgiou Hospital,
82,KID,2017,,diagnosis,medical,,images;video,textual descriptions;pixel-wise segmentation,2500,, A capsule endoscopy database for medical decision support.,http://is-innovation.eu/kid/login.php,,KID Project: an internet-based digital video atlas of capsule endoscopy for research purposes,"Koulaouzidis, A., Iakovidis, D. K., Yung, D. E., Rondonotti, E., Kopylov, U., Plevris, J. N., ... & Mavrogenis, G. (2017). KID Project: an internet-based digital video atlas of capsule endoscopy for research purposes. Endoscopy international open, 5(6), E477.",,,
83,LITS Liver Tumor Segmentation,2017,,diagnosis,medical,tumor,images,pixel-wise segmentation ,200,,200 CT scans of liver and tumors with segmentation annotations.,https://competitions.codalab.org/competitions/17094#learn_the_details,https://codalabcmpprod.blob.core.windows.net/pub/logos/logo_807.jpg,,,,,
84,MIT CBCL Automated Mouse Behavior Recognition datasets,2010,"H. Jhuang, E. Garrote, X. Yu, V. Khilnani, T. Poggio, A. Steele and T. Serre",recognition;classification,action;behavior ,rodent,images;video,category labels,4200,,Over ten hours of continuous labeled mice behavior. The clipped database includes 4200 short clips of the most unambiguous examples of each behavior.,http://cbcl.mit.edu/software-datasets/mouse/,,Automated Home-Cage Behavioral Phenotyping of Mice,"H. Jhuang, E. Garrote, X. Yu, V. Khilnani, T. Poggio, A. Steele and T. Serre. Automated Home-Cage Behavioral Phenotyping of Mice. Nature communications, Sep., 2010 ",,MIT,
85,Fine-grained Recognition Datasets for Biodiversity Analysis,2015,"Erik Rodner, Marcel Simon, Gunnar Brehm, Stephanie Pietsch, J. Wolfgang Wägele, Joachim Denzler",classification,biological,insect,images,category labels,675,,Two datasets featuring moths and butterflies with artificially spread wings for biodiversity analysis.,http://www.inf-cv.uni-jena.de/fgvcbiodiv,http://www.inf-cv.uni-jena.de/dbvmedia/de/Bilder/FormerMembers/Erik/dataset_costarica_small.png,Fine-grained recognition datasets for biodiversity analysis,"Rodner, E., Simon, M., Brehm, G., Pietsch, S., Wägele, J. W., & Denzler, J. (2015). Fine-grained recognition datasets for biodiversity analysis. arXiv preprint arXiv:1507.00913.",CVPR,Jena University,
86,Mouse Embryo Tracking Database ,2014,Marcelo Cicconet and Kris Gunsalus,tracking,biological,rodent;cells,video frames,category labels;localization line/shape coordinates,100,8,A cell-tracking database containing 100 annotated examples of mouse embryos up to the 8-cell stage.,http://celltracking.bio.nyu.edu/,http://celltracking.bio.nyu.edu/Frame4.png,Label free cell-tracking and division detection based on 2D time-lapse images for lineage analysis of early embryo development,"Cicconet, M., Gutwein, M., Gunsalus, K. C., & Geiger, D. (2014). Label free cell-tracking and division detection based on 2D time-lapse images for lineage analysis of early embryo development. Computers in biology and medicine, 51, 24-34.",,,
87,SCORHE ,2014,,classification;recognition,biological;behavior,rodent,video,temporal category labels,,9,SCORHE (System for Continuous Observation of Rodents in Home-cage Environment) is comprised of custom video-acquisition and analysis tools developed to quantify mice activity and behavior for short and long (multi-day) durations while housed within a typical home-cage. ,https://spis.cit.nih.gov/node/30,https://spis.cit.nih.gov/sites/default/files/styles/very_large/public/SCORHE_Simulation.png?itok=My4Who5D,SCORHE: A novel and practical approach to video monitoring of laboratory mice housed in vivarium cage racks,"Salem, G. H., Dennis, J. U., Krynitsky, J., Garmendia-Cedillos, M., Swaroop, K., Malley, J. D., ... & Gottesman, M. M. (2015). SCORHE: A novel and practical approach to video monitoring of laboratory mice housed in vivarium cage racks. Behavior research methods, 47(1), 235-250.",,,
88,GoPro-Gyro Dataset ,2015,Hannes Ovrén and Per-Erik Forssén,camera analysis,video,gyroscope,video,gyroscope measurements,,,This dataset consists of a number of wide-angle rolling shutter video sequences with corresponding gyroscope measurements.,http://www.cvl.isy.liu.se/en/research/datasets/gopro-gyro-dataset/,http://www.cvl.isy.liu.se/en/research/datasets/gopro-gyro-dataset/webpage_sequences.png, Gyroscope-based video stabilisation with auto-calibration. In Robotics and Automation ,"Ovrén, H., & Forssén, P. E. (2015, May). Gyroscope-based video stabilisation with auto-calibration. In Robotics and Automation (ICRA), 2015 IEEE International Conference on(pp. 2090-2097). IEEE.",,Linkoping University,
89,300 Videos in the Wild (300-VW),2015,"Grigorios G. Chrysos, Epameinondas Antonakos, Stefanos Zafeiriou, Patrick Snape",detection;localization,faces,,video frames,landmark annotations,33000,,"In order to develop a comprehensive benchmark for evaluating facial landmark tracking algorithms in the wild, we have collected a large number of long facial videos recorded in the wild. Each video has duration of ~1 minute (at 25-30 fps).",https://ibug.doc.ic.ac.uk/resources/300-VW/,https://ibug.doc.ic.ac.uk/media/uploads/images/scenario_1_003_000055.png, Offline deformable face tracking in arbitrary videos,"G. S. Chrysos, E. Antonakos, S. Zafeiriou and P. Snape. Offline deformable face tracking in arbitrary videos. In IEEE International Conference on Computer Vision Workshops (ICCVW), 2015. IEEE, 2015;J.Shen, S.Zafeiriou, G. S. Chrysos, J.Kossaifi, G.Tzimiropoulos, and M. Pantic. The first facial landmark tracking in-the-wild challenge: Benchmark and results. In IEEE International Conference on Computer Vision Workshops (ICCVW), 2015. IEEE, 2015;G. Tzimiropoulos. Project-out cascaded regression with an application to face alignment. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3659–3667, 2015",ICCV,Imperial College London,
90,3D Mask Attack Database (3DMAD),2013,Nesli Erdogmus and Sébastien Marcel,recognition;authentication/matching,faces,spoofing,video frames;depth images;images,localization line/shape coordinates,76500,,"The 3D Mask Attack Database (3DMAD) is a biometric (face) spoofing database. It currently contains 76500 frames of 17 persons, recorded using Kinect for both real-access and spoofing attacks. ",https://www.idiap.ch/dataset/3dmad,https://www.idiap.ch/dataset/3dmad/fig_masks.png,Spoofing in 2D face recognition with 3D masks and anti-spoofing with Kinect,"Erdogmus, N., & Marcel, S. (2013, September). Spoofing in 2D face recognition with 3D masks and anti-spoofing with Kinect. In Biometrics: Theory, Applications and Systems (BTAS), 2013 IEEE Sixth International Conference on (pp. 1-6). IEEE.",,Idiap Research Institute,
91,MOBIO,2012,"Chris McCool, Sébastien Marcel, Abdenour Hadid, Matti Pietikäinen, Pavel Mat?jka, Jan ?ernocký, Norman Poh, Josef Kittler, Anthony Larcher, Christophe Lévy, Driss Matrouf, Jean-François Bonastre, Phil Tresadern, and Timothy Cootes",recognition,faces,,video;audio,textual descriptions,152,,Database of mobile phone video and audio for face and speaker recognition.,https://www.idiap.ch/dataset/mobio,https://www.idiap.ch/dataset/mobio/video1.jpg,Bi-Modal Person Recognition on a Mobile Phone: using mobile phone data,"Chris McCool, Sébastien Marcel, Abdenour Hadid, Matti Pietikäinen, Pavel Mat?jka, Jan ?ernocký, Norman Poh, Josef Kittler, Anthony Larcher, Christophe Lévy, Driss Matrouf, Jean-François Bonastre, Phil Tresadern, and Timothy Cootes, “Bi-Modal Person Recognition on a Mobile Phone: using mobile phone data”, in IEEE ICME Workshop on Hot Topics in Mobile Mutlimedia, 2012.",ICME,Idiap Research Institute;University of Oulu;Brno University of Technology;University of Surrey;University of Avignon;University of Manchester,
92,RatSI: Rat Social Interaction Dataset,2017,M. Lorbach; E. I. Kyriakou; R. Poppe; E. A. van Dam; L. P. J. J. Noldus & R. C. Veltkamp ,tracking;classification,vehicle,rodent,video;video frames,localization line/shape coordinates;category labels,9,10,The Rat Social Interaction (RatSI) dataset comprises 9 fully annotated videos of two rats interacting socially in a 90x90cm Noldus PhenoTyper™ 9000 cage without bedding and accessories. The recordings are made from top-view perspective and are about 15 minutes long. We provide both tracking data and frame-by-frame annotations that were scored by an expert in rat behavior.,http://www.noldus.com/projects/phenorat/datasets/ratsi,https://youtu.be/rHmR-Oq5d54,Learning to Recognize Rat Social Behavior: Novel Dataset and Cross-Dataset Application,M. Lorbach;E. I. Kyriakou;R. Poppe;E. A. van Dam;L. P. J. J. Noldus & R. C. Veltkamp (2017). Learning to Recognize Rat Social Behavior: Novel Dataset and Cross-Dataset Application. Journal of Neuroscience Methods.,,Utrecht University;Radboud University Medical Centre;Noldus Information Technology BV,
93,Retinal fundus images - Ground truth of vascular bifurcations and crossovers ,2011,G. Azzopardi and N. Petkov,diagnosis,medical,retina;vasculature,images,category labels;pixel-wise segmentation,40,,We provide the ground truth of the vascular features (bifurcations and crossovers) of 40 segmented retinal fundus images of the DRIVE (Digital Retinal Images for Vessel Extraction) data set,http://www.cs.rug.nl/~imaging/databases/retina_database/retinalfeatures_database.html,http://www.cs.rug.nl/~imaging/databases/retina_database/thmb/01_manual1_gt.png,,"Azzopardi, G., & Petkov, N. (2011). Detection of retinal vascular bifurcations by trainable V4-like filters. In Computer Analysis of Images and Patterns (pp. 451-459). Springer, Berlin, Heidelberg.",CAIP,University of Groningen,
94,VascuSynth,2013,Ghassan Hamarneh and Preet Jassi,diagnosis,medical,vasculature,3D images,pixel-wise segmentation,120,,"Simulated volumetric images of vascular trees and generate the corresponding ground truth segmentations, bifurcation locations, branch properties, and tree hierarchy",http://vascusynth.cs.sfu.ca/Welcome.html,http://vascusynth.cs.sfu.ca/Data_files/shapeimage_4.png,VascuSynth: simulating vascular trees for generating volumetric image data with ground-truth segmentation and tree analysis,"Hamarneh, G., & Jassi, P. (2010). VascuSynth: simulating vascular trees for generating volumetric image data with ground-truth segmentation and tree analysis. Computerized medical imaging and graphics, 34(8), 605-616.;Jassi, P., & Hamarneh, G. (2011). Vascusynth: Vascular tree synthesis software. Insight Journal.",,Simon Fraser University,
95,Catadioptric camera video dataset for vehicle classification,2012,"Karaimer, H.C. and Bastanlar, Y.",detection;classification,vehicle,,video;video frames,masks;pixel-wise segmentation;category labels,277,3,"For each video the following data is included in the zip files: i) the video itself (avi format), ii) the foreground mask of each frame obtained with background subtraction, iii) the foreground mask of each frame containing the vehicle, iv) annotated area covered by the vehicle (to be used as groundtruth) while vehicle is at the closest point to the camera.",http://cvrg.iyte.edu.tr/datasets.htm,, Detection and classification of vehicles from omnidirectional videos using temporal average of silhouettes,"Karaimer, H. C., & Bastanlar, Y. (2015). Detection and classification of vehicles from omnidirectional videos using temporal average of silhouettes.",,izmir Institute of Technology,
96,Omnidirectional and panoramic image dataset for human and car detection,2014,"Ibrahim Cinaroglu, Yalin Bastanlar",detection,vehicle;person,,images,bounding boxes,290,,Omnidirectional images to detect humans and cars.,http://cvrg.iyte.edu.tr/datasets.htm,,A Direct Approach for Human Detection with Catadioptric Omnidirectional Cameras,"Cinaroglu, I. and Bastanlar, Y. (2014), A Direct Approach for Human Detection with Catadioptric Omnidirectional Cameras, IEEE Conference on Signal Processing and Communications Applications (SIU) 2014.",,izmir Institute of Technology,
97,Panoramic image dataset for car detection,2014,"Karaimer, H.C. and Bastanlar, Y.",detection,vehicle,,images,bounding boxes,125,,"Dataset contains 25 para-catadioptric images, ii) 50 cylindrical panoramic images obtained from the catadioptric images (25 original and 25 mirrored), annotations for cars in those images, iii) 50 spherical panoramic images obtained from the catadioptric images, annotations for cars in those images",http://cvrg.iyte.edu.tr/datasets.htm,,Car Detection with Omnidirectional Cameras Using Haar-like Features and Cascaded Boosting," Karaimer, H.C. and Bastanlar, Y. (2014), Car Detection with Omnidirectional Cameras Using Haar-like Features and Cascaded Boosting (in Turkish), IEEE Conference on Signal Processing and Communications Applications (SIU) 2014.",,izmir Institute of Technology,
100,BioID face database,2018,,detection,faces;eyes,,images;grayscale images,localization line/shape coordinates;landmark annotations,1521,,"The dataset consists of 1521 gray level images with a resolution of 384×286 pixel. Each one shows the frontal view of a face of one out of 23 different test persons. For comparison reasons the set also contains manually set eye postions. The images are labeled “BioID_xxxx.pgm” where the characters xxxx are replaced by the index of the current image (with leading zeros). Similar to this, the files “BioID_xxxx.eye” contain the eye positions for the corresponding images.",https://www.bioid.com/facedb/,https://www.bioid.com/wp-content/uploads/face-database-bioid.jpg,,,,BioID,
101,Biwi 3D Audiovisual Corpus of Affective Communication ,2010,"G. Fanelli, J. Gall, H. Romsdorfer, T. Weise and L. Van Gool",recognition;tracking;classification,faces;speech/language;affect/expression,,video;3D mesh;audio,temporal localization line/shape coordinates;audio segmentation;affect annotations,1109,15,The corpus comprises a total of 1109 sentences uttered by 14 native English speakers (6 males and 8 females). A real time 3D scanner and a professional microphone were used to capture the facial movements and the speech of the speakers.,http://www.vision.ee.ethz.ch/datasets/b3dac2.en.html,http://www.vision.ee.ethz.ch/datasets/B3DAC2/setup.jpg,A 3D Audio-Visual Corpus of Affective Communication ,"Fanelli, G., Gall, J., Romsdorfer, H., Weise, T., & Van Gool, L. (2010). A 3-d audio-visual corpus of affective communication. IEEE Transactions on Multimedia, 12(6), 591-598.;Fanelli, G., Gall, J., Romsdorfer, H., Weise, T., & Van Gool, L. (2010, May). 3D vision technology for capturing multimodal corpora: chances and challenges. In Proceedings of the LREC Workshop on Multimodal Corpora (pp. 70-73).",LREC ,ETH Zürich,
102,"Bosphorus 3D/2D Database of FACS annotated facial expressions, of head poses and of face occlusions",2009,,identification;recognition;detection;modeling;reconstruction;classification,faces;affect/expression,,images;3D images,category labels,4666,35,"The Bosphorus Database is intended for research on 3D and 2D human face processing tasks including expression recognition, facial action unit detection, facial action unit intensity estimation, face recognition under adverse conditions, deformable face modeling, and 3D face reconstruction. There are 105 subjects and 4666 faces in the database. ",http://bosphorus.ee.boun.edu.tr/default.aspx,http://bosphorus.ee.boun.edu.tr/Portals/0/Images/imtab_mixed1_files_small.jpg,3D Face Recognition Performance Under Adversorial Conditions,"A. Savran, O. Çeliktutan, A. Akyol, J. Trojanova, H. Dibeklio?lu, S. Esenlik, N. Bozkurt, C. Demirk?r, E. Akagündüz, K. Çal??kan, N.Alyüz, B.Sankur, ?. Ulusoy, L. Akarun, T. M. Sezgin, ""3D Face Recognition Performance Under Adversorial Conditions"", in Proc. eNTERFACE’07 Workshop on Multimodal Interfaces, Istanbul, Turkey, July 2007.",,Bogazici University,
103,Caricature/Photomates dataset ,2015,"Bahri Abaci, Tayfun Akgul",recognition,faces,cartoon,images,identity annotations,200,,A publicly available caricature-photograph database with 200 caricatures and corresponding photomates,http://www.gag.itu.edu.tr/CPdatabase/,http://www.gag.itu.edu.tr/CPdatabase/index_files/image002.png,Matching caricatures to photographs,"Abaci, B., & Akgul, T. (2015). Matching caricatures to photographs. Signal, Image and Video Processing, 9(1), 295-303.",,Istanbul Technical University,
104,CASIA-IrisV3,2010,"Tieniu Tan, Zhenan Sun",recognition,eyes,iris,images,n/a,22035,," CASIA-IrisV3 includes three subsets which are labeled as CASIA-IrisV3-Interval, CASIA-IrisV3-Lamp, CASIA-IrisV3-Twins. CASIA-IrisV3 contains a total of 22,035 iris images from more than 700 subjects. All iris images are 8 bit gray-level JPEG files, collected under near infrared illumination. ",http://www.cbsr.ia.ac.cn/IrisDatabase.htm,http://www.cbsr.ia.ac.cn/IrisDatabase/IrisDatabase_clip_image005.gif,,,,Institute of Automation chinese Academy of Sciences,
105,CASIR Gaze Estimation Database ,2015,João Filipe Ferreira and Pablo Lanillos,gaze estimation;pose estimation,gaze,,images;depth images,gaze fixation annotations;landmark annotations,,,RGB and depth images (from Kinect V1.0) and ground truth values of facial features corresponding to experiments for gaze estimation benchmarking,http://mrl.isr.uc.pt/experimentaldata/public/gaze-casir/#changelog,,,,,,
106,The CMU Multi-PIE Face Database ,2009,"Ralph Gross, Iain Matthews, Jeffrey Cohn, Takeo Kanade, Simon Baker",recognition;classification,affect/expression,,images,category labels,750000,,"The CMU Multi-PIE face database contains more than 750,000 images of 337 people recorded in up to four sessions over the span of five months. Subjects were imaged under 15 view points and 19 illumination conditions while displaying a range of facial expressions. ",http://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.html,http://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home_files/252_03_02_051_00.jpg,Multi-PIE,"Gross, R., Matthews, I., Cohn, J., Kanade, T., & Baker, S. (2010). Multi-pie. Image and Vision Computing, 28(5), 807-813.",,Carnegie Mellon University;University of Pittsburgh;Microsoft Research,
107,The Extended Cohn-Kande Dataset (CK+),2010,"Lucey, P., Cohn, J. F., Kanade, T., Saragih, J., Ambadar, Z., & Matthews, I.",recognition;classification,affect/expression,,images,category labels,593,7,Dataset for research into automatically detecting individual facial expressions.,http://www.pitt.edu/~emotion/ck-spread.htm,http://www.pitt.edu/~emotion/images/CKimage.png,The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression.,"Lucey, P., Cohn, J. F., Kanade, T., Saragih, J., Ambadar, Z., & Matthews, I. (2010, June). The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE Computer Society Conference on (pp. 94-101). IEEE.",CVPR,Carnegie Mellon University;University of Pittsburgh,
108,Columbia Gaze Data Set ,2013,"Brian A. Smith,  Qi Yin,  Steven K. Feiner,  Shree K. Nayar",gaze estimation,gaze,head,images,gaze fixation annotations;landmark annotations,5880,,"We have created a large publicly available gaze data set: 5,880 images of 56 people over varying gaze directions and head poses. We created this data set to train a detector to sense eye contact in an image using a passive, appearance-based approach. However, the data set can be used for many other gaze estimation and tracking purposes as well. ",http://www.cs.columbia.edu/CAVE/databases/columbia_gaze/,http://www.cs.columbia.edu/CAVE/databases/columbia_gaze/gaze_database.png,Gaze Locking: Passive Eye Contact Detection for Human–Object Interaction,"Smith, B. A., Yin, Q., Feiner, S. K., & Nayar, S. K. (2013, October). Gaze locking: passive eye contact detection for human-object interaction. In Proceedings of the 26th annual ACM symposium on User interface software and technology(pp. 271-280). ACM.",UIST,Columbia University,
109,AffectNet,2017,"Ali Mollahosseini, Behzad Hasani, and Mohammad H. Mahoor",recognition;classification,affect/expression,,images,category labels;valence/intensity annotations,1000000,7,"AffectNet is a new database of facial expressions in the wild, by collecting and annotating facial images. AffectNet contains more than 1M facial images collected from the Internet by querying three major search engines using 1250 emotion related keywords in six different languages. ",http://mohammadmahoor.com/affectnet/,http://mohammadmahoor.com/wp-content/uploads/2017/06/sample2-1-768x833.png,"Affectnet: A database for facial expression, valence/intensity annotation, and arousal computing in the wild","Mollahosseini, A., Hasani, B., & Mahoor, M. H. (2017). Affectnet: A database for facial expression, valence/intensity annotation, and arousal computing in the wild. arXiv preprint arXiv:1708.03985.",,University of Denver,
110,DISFA:Denver Intensity of Spontaneous Facial Action Database ,2013,"S.Mohammad Mavadati, Mohammad H. Mahoor, Kevin Bartlett, Philip Trinh, and Jeffrey F. Cohn",recognition;classification,affect/expression,,images,category labels;landmark annotations;valence/intensity annotations,130000,12,The Denver Intensity of Spontaneous Facial Action Database is a non-posed facial expression database for those who are interested in developing computer algorithms for automatic action unit detection and their intensities described by FACS. This database contains stereo videos of 27 adult subjects (12 females and 15 males) with different ethnicities. The images were acquired using PtGrey stereo imaging system at high resolution (1024×768). The intensity of AU’s (0-5 scale) for all video frames were manually scored by two human FACS experts. The database also includes 66 facial landmark points of each image in the database. ,http://mohammadmahoor.com/disfa/,,Disfa: A spontaneous facial action intensity database,"Mavadati, S. M., Mahoor, M. H., Bartlett, K., Trinh, P., & Cohn, J. F. (2013). Disfa: A spontaneous facial action intensity database. IEEE Transactions on Affective Computing, 4(2), 151-160.",,University of Denver;Carnegie Mellon University;University of Pittsburgh,
111,DISFA+:Extended Denver Intensity of Spontaneous Facial Action Database ,2016,"Mohammad Mavadati, Peyten Sanger, Mohammad H. Mahoor",recognition;classification,affect/expression,,images,category labels;landmark annotations;valence/intensity annotations,,12,"The Extended Denver Intensity of Spontaneous Facial Action Database is an  extension of DISFA, a previously released and well-accepted face dataset. Extended DISFA (DISFA+) has the following features: 1) it contains a large set of posed and non-posed facial expressions data for a same group of individuals, 2) it provides the manually labeled frame-based annotations of 5-level intensity of twelve FACS facial actions, 3) it provides meta data (i.e. facial landmark points in addition to the self-report of each individual regarding every posed facial expression). ",http://mohammadmahoor.com/disfa/,,Extended DISFA Dataset: Investigating Posed and Spontaneous Facial Expressions,"Mavadati, M., Sanger, P., & Mahoor, M. H. (2016). Extended DISFA Dataset: Investigating Posed and Spontaneous Facial Expressions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (pp. 1-8).",CVPR,University of Denver,
112,EURECOM Facial Cosmetics Database,2013,"Marie-Lena Eckert, Neslihan Kose, and Jean-Luc Dugelay",recognition;classification,faces,cosmetics,images,valence/intensity annotations;category labels;localization line/shape coordinates,389,8,"The database contains multiple images with and without applied makeup per person and consists in total of 389 images and 50 persons. Each image is annotated with the amount and location of applied makeup i.e. the makeup code. With this code, it is possible to deduct the makeup category automatically. For each person, one reference image with no makeup is stored and one or more makeup series.",http://fcd.eurecom.fr/,http://fcd.eurecom.fr/files/Bild3_2_0.png,Facial Cosmetics Database and Impact Analysis on Automatic Face Recognition,"Eckert, M. L., Kose, N., & Dugelay, J. L. (2013, September). Facial cosmetics database and impact analysis on automatic face recognition. In Multimedia Signal Processing (MMSP), 2013 IEEE 15th International Workshop on (pp. 434-439). IEEE.",,TU Muenchen Boltzmannstr;EURECOM,
113,EURECOM Kinect Face Database ,2014,"Rui Min, Neslihan Kose, Jean-Luc Dugelay",recognition;classification,faces;affect/expression,,images;depth images;3D images,landmark annotations;category labels;attribute annotations,"Min, R., Kose, N., & Dugelay, J. L. (2014). Kinectfacedb: A kinect database for face recognition. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 44(11), 1534-1548.",9,"The Dataset consists of the multimodal facial images of 52 people (14 females, 38 males) obtained by Kinect. The data is captured in two sessions happened at different time period (about half month). In each session, the dataset provides the facial images of each person in 9 states of different facial expressions, different lighting and occlusion conditions.",http://rgb-d.eurecom.fr/,http://rgb-d.eurecom.fr/files/exampledb-crop.png,KinectFaceDB: A Kinect Database for Face Recognition,"Min, R., Kose, N., & Dugelay, J. L. (2014). Kinectfacedb: A kinect database for face recognition. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 44(11), 1534-1548.",,University of North Carolina at Chapel Hill;EUROCOM,
114,EYEDIAP dataset ,2014,"Funes Mora, Kenneth Alberto and Monay, Florent and Odobez, Jean-Marc",gaze estimation,gaze,,video;RGB-D video,gaze fixation anootaions;joint positions/pose annotations,94,,"The EYEDIAP dataset intends to fill the need for a standard database for gaze estimation from remote RGB, and RGB-D (standard vision and depth), cameras.",https://www.idiap.ch/dataset/eyediap,,EYEDIAP: A Database for the Development and Evaluation of Gaze Estimation Algorithms from RGB and RGB-D Cameras,"Funes Mora, K. A., Monay, F., & Odobez, J. M. (2014, March). Eyediap: A database for the development and evaluation of gaze estimation algorithms from rgb and rgb-d cameras. In Proceedings of the Symposium on Eye Tracking Research and Applications (pp. 255-258). ACM.",,Idiap Research Institute;Ecole Polytechnique Federale de Lausanne,
115,FDDB: Face Detection Data set and Benchmark - studying unconstrained face detection,2010,Vidit Jain and Erik Learned-Miller,detection,faces,,images,bounding boxes,5171,,A data set of face regions designed for studying the problem of unconstrained face detection. This data set contains the annotations for 5171 faces in a set of 2845 images taken from the Faces in the Wild data set.,http://vis-www.cs.umass.edu/fddb/,,FDDB: A Benchmark for Face Detection in Unconstrained Settings,"Jain, V., & Learned-Miller, E. (2010). Fddb: A benchmark for face detection in unconstrained settings. University of Massachusetts, Amherst, Tech. Rep. UM-CS-2010-009, 2(7), 8.",,University of Massachusetts Amherst,
116,FaceScrub,2014,"H.-W. Ng, S. Winkler.",detection;recognition,faces,,images,identity annotations,106863,,"FaceScrub comprises a total of 106,863 face images* of male and female 530 celebrities, with about 200 images per person. As such, it is one of the largest public face databases.",http://vintage.winklerbros.net/facescrub.html,http://vintage.winklerbros.net/Images/facescrub.jpg,A data-driven approach to cleaning large face datasets,"Ng, H. W., & Winkler, S. (2014, October). A data-driven approach to cleaning large face datasets. In Image Processing (ICIP), 2014 IEEE International Conference on (pp. 343-347). IEEE.",ICIP,University of Illinois at Urbana-Champaign,
117,Facial Expression Dataset (AM-FED),2011,"Daniel McDuff, Rana el Kaliouby, Thibaud Senechal, May Amr, Jeffrey Cohn, Rosalind Picard and Affectiva",detection;recognition,faces;affect/expression,,video;video frames,temporal category labels;tracking/movement annotations;attribute annotations;landmark annotations;valence/intensity annotations,168359,10,"This dataset consists of 242 facial videos (168,359 frames) recorded in real world conditions.",https://www.affectiva.com/facial-expression-dataset/,,Naturalistic and Spontaneous Facial Expressions Collected “In-the-Wild”(2013),"Affectiva-MIT Facial Expression Dataset (AM-FED): Naturalistic and Spontaneous Facial Expressions Collected “In-the-Wild”(2013). Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops;McDuff, D., Kaliouby, R., and Picard, R. Crowdsourcing Facial Responses to Online Videos. IEEE Transactions on Affective Computing, 2012.;McDuff, D., el Kaliouby, R. and Picard, R. Crowdsourced Data Collection of Facial Responses. Proceedings of the 13th international conference on multimodal interfaces, 2011.;McDuff, D., el Kaliouby, R. Demirdjian D., and Picard, R. Predicting Online Media Effectiveness Based on Smile Responses Gathered Over the Internet. Proceedings of the 10th international conference on Automatic Face and Gesture Recognition, 2013.",CVPR,Affectiva Inc.;MIT Media Lab;Carnegie Mellon University,
118,Florence 2D/3D Hybrid Face Dataset ,2011,"Bagdanov, A. D., Del Bimbo, A., & Masi, I.",recognition;pose estimation,faces,,video;images;3D models;3D mesh,n/a,,,"The dataset consists of high-resolution 3D scans of human faces along with several video sequences of varying resolution and zoom level. Each subject is recorded under various scenarios, settings and conditions. This dataset is being constructed specifically to support research on techniques that bridge the gap between 2D, appearance-based recognition techniques, and fully 3D approaches. It is designed to simulate, in a controlled fashion, realistic surveillance conditions and to probe the efficacy of exploiting 3D models in real scenarios.",https://www.micc.unifi.it/resources/datasets/florence-3d-faces/,https://www.micc.unifi.it/wp-content/uploads/2016/01/superfaces-750x400.png,The Florence 2D/3D Hybrid Face Dataset,"Bagdanov, A. D., Del Bimbo, A., & Masi, I. (2011, December). The Florence 2D/3D hybrid face dataset. In Proceedings of the 2011 joint ACM workshop on Human gesture and behavior understanding (pp. 79-80). ACM.",,Media Integration and Communication Center,
119,Gi4E Database ,2013,"Arantxa Villanueva, Victoria Ponz, Laura Sesma?Sanchez, Mikel Ariz, Sonia Porta, and Rafael Cabeza",detection;localization,eyes,iris,images,localization line/shape coordinates,1339,,"We have created a public database of images for iris center and eye corner detection. The database consists of a set of 1339 images acquired with a standard webcam, corresponding to 103 different subjects and 13 images each. Every set of 13 images consists of 12 images in which the user gazes at different points in the screen and another image in which the user’s eyes are closed.",http://gi4e.unavarra.es/databases/gi4e/,http://gi4e.unavarra.es/gi4e/imgs/sample_3.png,Hybrid method based on topography for robust detection of iris center and eye corners,"Villanueva, A., Ponz, V., Sesma-Sanchez, L., Ariz, M., Porta, S., & Cabeza, R. (2013). Hybrid method based on topography for robust detection of iris center and eye corners. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 9(4), 25.",,,
120,IMDB-WIKI,2015,"Rasmus Rothe, Radu Timofte, Luc Van Gool",regression,faces,age,images,category labels,500000,,The IMDB-WIKI dataset contains more than 500k face images with gender and age labels for training. ,http://www.vision.ee.ethz.ch/en/datasets/,https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/img/imdb-wiki-teaser.png,DEX: Deep EXpectation of apparent age from a single image,"Rasmus Rothe and Radu Timofte and Luc Van Gool, ""DEX: Deep EXpectation of apparent age from a single image"", ICCVW, 2015.;Rasmus Rothe and Radu Timofte and Luc Van Gool, ""Deep expectation of real and apparent age from a single image without facial landmarks"", IJCV, 2016.",ICCV;IJCV,"D-ITET, ETH Zurich",
121,Indian Movie Face database (IMFDB),2013,"Shankar Setty, Moula Husain, Parisa Beham, Jyothi Gudavalli, Menaka Kandasamy, Radhesyam Vaddi, Vidyagouri Hemadri, J C Karure, Raja Raju, Rajan, Vijay Kumar and C V Jawahar",recognition,faces;affect/expression,movie,images,attribute annotations;category labels,34512,,"Indian Movie Face database (IMFDB) is a large unconstrained face database consisting of 34512 images of 100 Indian actors collected from more than 100 videos. All the images are manually selected and cropped from the video frames resulting in a high degree of variability interms of scale, pose, expression, illumination, age, resolution, occlusion, and makeup. ",http://cvit.iiit.ac.in/projects/IMFDB/,http://cvit.iiit.ac.in/projects/IMFDB/faces5.png,Indian Movie Face Database: A Benchmark for Face Recognition Under Wide Variations,"Setty, S., Husain, M., Beham, P., Gudavalli, J., Kandasamy, M., Vaddi, R., ... & Kumar, V. (2013, December). Indian movie face database: a benchmark for face recognition under wide variations. In Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG), 2013 Fourth National Conference on (pp. 1-5). IEEE.",NCVPRIPG,"BVBCET;VRS College;Vickramce, MDU;TCE;PSYEC;SDM;CVIT; IIITH",
122,LFW: Labeled Faces in the Wild,2016,"Erik Learned-Miller, Gary B. Huang, Aruni RoyChowdhury, Haoxiang Li, and Gang Hua.",recognition;identification,faces,,images,identity annotations,13000,,"Labeled Faces in the Wild is a database of face photographs designed for studying the problem of unconstrained face recognition. The data set contains more than 13,000 images of faces collected from the web. Each face has been labeled with the name of the person pictured. ",http://vis-www.cs.umass.edu/lfw/,http://vis-www.cs.umass.edu/lfw/Six_Face_Panels_sm.jpg,Labeled Faces in the Wild: A Survey.,"Learned-Miller, E., Huang, G. B., RoyChowdhury, A., Li, H., & Hua, G. (2016). Labeled faces in the wild: A survey. In Advances in face detection and facial image analysis (pp. 189-248). Springer, Cham.",,"University of Massachusetts, Amherst;Howard Hughes Medical Institute;Stevens Institute of Technology",
123,YouTube Makeup (YMU),2012,"A. Dantcheva, C. Chen, A. Ross",recognition,faces,cosmetics,video;video frames;images,identity annotations,604,,"YMU is a dataset consisting of 151 subjects, specifically Caucasian females, from YouTube makeup tutorials. We captured images of the subjects before and after the application of makeup. There are four shots per subject: two shots before the application of makeup and two shots after the application of makeup",http://www.antitza.com/makeup-datasets.html,,Can Facial Cosmetics Affect the Matching Accuracy of Face Recognition Systems?,"A. Dantcheva, C. Chen, A. Ross, ""Can Facial Cosmetics Affect the Matching Accuracy of Face Recognition Systems?,"" Proc. of 5th IEEE International Conference on Biometrics: Theory, Applications and Systems (BTAS), (Washington DC, USA), September 2012.; C. Chen, A. Dantcheva, A. Ross, ""Automatic Facial Makeup Detection with Application in Face Recognition,"" Proc. of 6th IAPR International Conference on Biometrics (ICB), (Madrid, Spain), June 2013.",BTAS;ICB,West Virginia University;Michigan State University,
124,Makeup Induced Face Spoofing (MIFS),2017,"C. Chen, A. Dantcheva, T. Swearingen, A. Ross",authentication/matching;recognition,faces,spoofing;cosmetics,images,identity annotations,642,,MIFS is a dataset consisting of 107 makeup-transformations taken from random YouTube makeup video tutorials. Each subject is attempting to spoof a target identity. Hence we provide three sets of face images: images of a subject before makeup; images of the same subject after makeup with the intention of spoofing; and images of the target subject who is being spoofed.,http://www.antitza.com/makeup-datasets.html,http://www.antitza.com/img/MU_spoofig.png,Spoofing Faces Using Makeup: An Investigative Study,"C. Chen, A. Dantcheva, T. Swearingen, A. Ross, ""Spoofing Faces Using Makeup: An Investigative Study,"" Proc. of 3rd IEEE International Conference on Identity, Security and Behavior Analysis (ISBA), (New Delhi, India), February 2017.",IBSA,Michigan State University;Inria Mediterranee,
125,"Makeup in the ""Wild"" (MIW)",2013,"C. Chen, A. Dantcheva, T. Swearingen, A. Ross",detection;classification,faces,cosmetics,images,category labels;localization line/shape coordinates;bounding boxes,154,,MIW contains images obtained from the internet of faces with and without makeup.,http://www.antitza.com/makeup-datasets.html,http://www.antitza.com/images/VMU.jpg,Automatic Facial Makeup Detection with Application in Face Recognition,"C. Chen, A. Dantcheva, A. Ross, ""Automatic Facial Makeup Detection with Application in Face Recognition,"" Proc. of 6th IAPR International Conference on Biometrics (ICB), (Madrid, Spain), June 2013.",,Michigan State University;Inria Mediterranee,
126,The MUCT Face Database,2010,S. Milborrow and J. Morkel and F. Nicolls,recognition,faces,,images,landmark annotations,3755,,"The MUCT database consists of 3755 faces with 76 manual landmarks. The database was created to provide more diversity of lighting, age, and ethnicity than currently available landmarked 2D face databases. ",http://www.milbo.org/muct/,http://www.milbo.org/muct/muct-examples-lores.jpg,The MUCT Landmarked Face Database,"Milborrow, S., Morkel, J., & Nicolls, F. (2010). The MUCT landmarked face database. Pattern Recognition Association of South Africa, 201(0).",,University of Cape Town,
127,MIT eye tracking database (1003 images) ,2009,Tilke Judd and Krista Ehinger and Fredo Durand and Antonio Torralba,gaze estimation,gaze,,images,gaze fixation annotations,1003,,Large database of eye tracking data.,http://people.csail.mit.edu/tjudd/WherePeopleLook/index.html,,Learning to Predict Where Humans Look,"Judd, T., Ehinger, K., Durand, F., & Torralba, A. (2009, September). Learning to predict where humans look. In Computer Vision, 2009 IEEE 12th international conference on(pp. 2106-2113). IEEE.",ICCV,MIT,
128,MMI Facial Expression Database ,2010,"Michel F. Valstar, Maja Pantic",recognition;classification,affect/expression,,video;audio;video frames,category labels,2900,,"The MMI Facial Expression Database is an ongoing project, that aims to deliver large volumes of visual data of facial expressions to the facial expression analysis community.",https://mmifacedb.eu/,https://ibug.doc.ic.ac.uk/media/uploads/images/research/mmifacedb_example_research_slide.png,"Induced disgust, happiness and surprise: an addition to the mmi facial expression database","Valstar, M., & Pantic, M. (2010, May). Induced disgust, happiness and surprise: an addition to the mmi facial expression database. In Proc. 3rd Intern. Workshop on EMOTION (satellite of LREC): Corpora for Research on Emotion and Affect (p. 65).",,Imperial College London;Twente University,
129,MORPH (Craniofacial Longitudinal Morphological Face Database) ,2009,"Rawls, A. W., & Ricanek, K. ",recognition,faces,,images,attribute annotations,55000,,"MORPH is a longitudinal face database, developed for age progression and age estimation research. This database is primarily used to solve age-related problems of facial recognition systems. The data corpus provides the largest set of publicly available longitudinal adult images with supporting metadata and is still expanding; longitudinal spans range from several days to over twenty years. The metadata provided aids in classification by age, gender, and race and includes other key parameters that affect aging appearance.",https://ebill.uncw.edu/C20231_ustores/web/classic/product_detail.jsp?PRODUCTID=8,,Development and optimization of a longitudinal age progression database.,"Rawls, A. W., & Ricanek, K. (2009, September). MORPH: Development and optimization of a longitudinal age progression database. In European Workshop on Biometrics and Identity Management (pp. 17-24). Springer, Berlin, Heidelberg.",,University of North Carolina Wilmington,
130,Appearance-based Gaze Estimation in the Wild (MPIIGaze),2015,"Xucong Zhang and Yusuke Sugano and Mario Fritz and Bulling, Andreas",gaze estimation,gaze,,images,gaze fixation annotations,213659,,"We present the MPIIGaze dataset that contains 213,659 images we collected from 15 participants during natural everyday laptop use over more than three months. ",https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild/,https://www.mpi-inf.mpg.de/fileadmin/_processed_/d/6/csm_MPIIGaze_Examples_8639a4f948.png,Appearance-based Gaze Estimation in the Wild,"Zhang, X., Sugano, Y., Fritz, M., & Bulling, A. (2015). Appearance-based gaze estimation in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 4511-4520).",CVPR,Max Planck Institute for Informatics,
131,MegaFace ,2017,"Ira Kemelmacher-Shlizerman, Steve Seitz, Daniel Miller, Evan Brossard",recognition,faces,,images,bounding boxes;identity annotations,690572,,Face recognition and verification performance under up to 1 million distractors,http://megaface.cs.washington.edu/participate/challenge.html,,The megaface benchmark: 1 million faces for recognition at scale,"Kemelmacher-Shlizerman, I., Seitz, S. M., Miller, D., & Brossard, E. (2016). The megaface benchmark: 1 million faces for recognition at scale. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 4873-4882).",CVPR,University of Washington,
132,MegaFace2,2016,Aaron Nech and Ira Kemelmacher-Shlizerman,recognition,faces,,images,identity annotations,672000,,"Training on 672K identities, and then testing recognition and verification performance under 1 million distractors. ",http://megaface.cs.washington.edu/participate/challenge2.html,,Level Playing Field For Million Scale Face Recognition,"Nech, A., & Kemelmacher-Shlizerman, I. (2017, July). Level playing field for million scale face recognition. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (pp. 3406-3415). IEEE.",CVPR,University of Washington,
133,Music video dataset ,2016,"Zhang, Shun and Gong, Yihong and Huang, Jia-Bin and Lim, Jongwoo and Wang, Jinjun and Ahuja, Narendra and Yang, Ming-Hsuan",detection;tracking,faces,,video;video frames,bounding boxes,117598,,"We contribute a new dataset consisting of 8 music videos from YouTube. We provide full annotations of 3,845 face tracklets and 117,598 face detections. The videos with ground-truth annotations can be downloaded in BaiduYun.",http://shunzhang.me.pn/papers/eccv2016/,http://shunzhang.me.pn/papers/eccv2016/Apink/0676.jpg,Tracking Persons-of-Interest via Adaptive Discriminative Features,"Zhang, S., Gong, Y., Huang, J. B., Lim, J., Wang, J., Ahuja, N., & Yang, M. H. (2016, October). Tracking persons-of-interest via adaptive discriminative features. In European Conference on Computer Vision (pp. 415-433). Springer, Cham.",ECCV,"Xi’an Jiaotong University;University of Illinois, Urbana-Champaign;Hanyang University;University of California, Merced",
134,"Notre Dame Image Database for Contact Lens
Detection In Iris Recognition",2013,Jay Doyle and Kevin Bowyer,detection;segmentation,eyes,,images,pixel-wise segmentation,7300,,"The images in this database were acquired with an LG 4000 and an IrisGuard AD100 iris camera. The images are 480x640 in size, and are intensity images acquired under near-IR illumination. The datasbase contains images of subjects in three conditions: (1) wearing cosmetic contact lenses1 , (2) wearing clear soft contact lenses2 , and (3) wearing no contact lenses3 . The database is separated into two datasets: Dataset I (LG4000) and Dataset II (AD100). Dataset I is constructed of a training set of 3000 images, with 10 equal folds for testing purposes, and a verification dataset set of 1200 images. Dataset I— is constructed of a training set of 600 images, with 10 equal folds for testing purposes, and a verification dataset set of 300 images.",https://sites.google.com/a/nd.edu/public-cvrl/data-sets,, Notre Dame Image Database for Contact Lens Detection in Iris Recognition,"Doyle, J., & Bowyer, K. W. (2014). Notre Dame Image Database for Contact Lens Detection in Iris Recognition—2013.",,University of Notre Dame,
135,OUI-Adience Faces,2014," E. Eidinger, R. Enbar, and T. Hassner",classification,faces,gender;age,images,category labels,26580,,"In order to facilitate the study of age and gender recognition, we provide a data set and benchmark of face photos. The data included in this collection is intended to be as true as possible to the challenges of real-world imaging conditions. In particular, it attempts to capture all the variations in appearance, noise, pose, lighting and more, that can be expected of images taken without careful preparation or posing. ",https://www.openu.ac.il/home/hassner/Adience/data.html,https://www.openu.ac.il/home/hassner/Adience/adience_ageandgender.png,Age and Gender Estimation of Unfiltered Faces,"Eran Eidinger, Roee Enbar, and Tal Hassner, Age and Gender Estimation of Unfiltered Faces, Transactions on Information Forensics and Security (IEEE-TIFS), special issue on Facial Biometrics in the Wild, Volume 9, Issue 12, pages 2170 - 2179, Dec. 2014;Gil Levi and Tal Hassner, Age and Gender Classification Using Convolutional Neural Networks, IEEE Workshop on Analysis and Modeling of Faces and Gestures (AMFG), at the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Boston, June 2015;Tal Hassner, Shai Harel*, Eran Paz* and Roee Enbar, Effective Face Frontalization in Unconstrained Images, IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Boston, June 2015",CVPR,The Open University of Israel,
136,Pandora,2017,"Guido Borghi, Marco Venturelli, Roberto Vezzani and Rita Cucchiara",pose estimation,person,shoulder pose;driving,images;depth images,depth positions;joint positions/pose annotations,250000,,"We collect a new challenging dataset, called Pandora. The dataset has been specifically created for the tasks described in the paper (i.e., head center localization, head pose and shoulder pose estimation) and is inspired by the automotive context. A frontal fixed device acquires the upper body part of the subjects, simulating the point of view of camera placed inside the dashboard. Among the others, the subjects also perform driving-like actions, such as grasping the steering wheel, looking to the rear-view or lateral mirrors, shifting gears and so on. Pandora contains 110 annotated sequences using 10 male and 12 female actors. Each subject has been recorded five times.",http://imagelab.ing.unimore.it/pandora/,http://imagelab.ing.unimore.it/pandora/assets/img/Immagine4.png,Poseidon: Face-from-depth for driver pose estimation.,"Borghi, G., Venturelli, M., Vezzani, R., & Cucchiara, R. (2017, July). Poseidon: Face-from-depth for driver pose estimation. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (pp. 5494-5503). IEEE.",CVPR,University of Modena and Reggio Emilia,
137,PubFig: Public Figures Face Database ,2009,"Neeraj Kumar, Alexander C. Berg, Peter N. Belhumeur, and Shree K. Nayar",identification,faces,,images,identity annotations,58797,,"The PubFig database is a large, real-world face dataset consisting of 58,797 images of 200 people collected from the internet. Unlike most other existing face datasets, these images are taken in completely uncontrolled situations with non-cooperative subjects. Thus, there is large variation in pose, lighting, expression, scene, camera, imaging conditions and parameters, etc.",http://www1.cs.columbia.edu/CAVE/databases/pubfig/,http://www1.cs.columbia.edu/CAVE/databases/pubfig/pubfig.png,Attribute and Simile Classifiers for Face Verification,"Kumar, N., Berg, A. C., Belhumeur, P. N., & Nayar, S. K. (2009, September). Attribute and simile classifiers for face verification. In Computer Vision, 2009 IEEE 12th International Conference on (pp. 365-372). IEEE.",ICCV,Columbia University,
138,SCface,2011,"Mislav Grgic, Kresimir Delac, Sonja Grgic, Bozidar Klimpak",identification,faces,,images,identity annotations,4160 ,,SCface is a database of static images of human faces. Images were taken in uncontrolled indoor environment using five video surveillance cameras of various qualities. Database contains 4160 static images (in visible and infrared spectrum) of 130 subjects. ,http://www.scface.org/,http://www.scface.org/SCface_coord.jpg,SCface – surveillance cameras face database,"Grgic, M., Delac, K., & Grgic, S. (2011). SCface–surveillance cameras face database. Multimedia tools and applications, 51(3), 863-879.",,University of Zagreb,
139,SiblingsDB,2014,"T.F. Vieira, A. Bottino, A. Laurentini, M. De Simone",recognition,faces,similarity;kinship,images,landmark annotations,184,,The SiblingsDB contains different datasets depicting images of individuals realted by sibling relationships.,https://areeweb.polito.it/ricerca/cgvg/siblingsDB.html,https://areeweb.polito.it/ricerca/cgvg/Projects/Kinship/siblingDB.png,Detecting Siblings in Image Pairs,"Vieira, T. F., Bottino, A., Laurentini, A., & De Simone, M. (2014). Detecting siblings in image pairs. The Visual Computer, 30(12), 1333-1345.",,Politecnico di Torino,
140,Spontaneous Emotion Multimodal Database (SEM-db),2016,"Juan Manuel Fernandez Montenegro, Athanasios Gkelias, V. Argyriou",recognition,affect/expression,,images;depth images;sensor recordings,gaze fixation annotations;landmark annotations,90,,SEM database is a multimodal dataset for spontaneous emotional reaction recognition that contains multimodal information of nine participants aged between 30 to 60 years old with different educational background taken while completing cognitive/visual tests. Ten repetitions have been recorded per participant providing a total of 90 instances.,http://staffnet.kingston.ac.uk/~ku43576/?page%20id=414,Emotion understanding using multimodal information based on autobiographical memories for Alzheimer’s patients,,"Montenegro, J. M. F., Gkelias, A., & Argyriou, V. (2016, November). Emotion understanding using multimodal information based on autobiographical memories for Alzheimer’s patients. In Asian Conference on Computer Vision(pp. 252-268). Springer, Cham.",ACCV,Kingston University;Imperial College,
141,The UNBC-McMaster Shoulder Pain Expression Archive Database,2011,"Patrick Lucey, Jeffrey F. Cohn, Kenneth M. Prkachin, Patricia E. Solomon and Iain Matthews",recognition,affect/expression,pain,video;video frames,landmark annotations;valence/intensity annotations,48398 ,,"Automatic pain detection dataset. To facilitate this work, researchers at McMaster University and University of Northern British Columbia captured video of participant's faces (who were suffering from shoulder pain) while they were performing a series of active and passive range-of-motion tests to their affected and unaffected limbs on two separate occasions. Each frame of this data was AU coded by certified FACS coders, and self-report and observer measures at the sequence level were taken as well.",http://www.pitt.edu/~emotion/um-spread.htm,,Painful data: The UNBC-McMaster Shoulder Pain Expression Archive Database,"Lucy, P., Cohn, J. F., Prkachin, K. M., Solomon, P., & Matthrews, I. (2011). Painful data: The UNBC-McMaster Shoulder Pain Expression Archive Database. IEEE International Conference on Automatic Face and Gesture Recognition (FG2011).",FG,"Carnegie Mellon University;University
of Northern British Columbia;McMaster University",
142,RGB-D Person Re-identification Dataset,2012,"B. I. Barbosa, M. Cristani, A. Del Bue, L. Bazzani, and V. Murino.",identification,person,,images;skeleton images;3D mesh,masks;depth positions;identification annotations,395,,"This dataset aims at promoting the RGB-D re-identification research using depth information. The proposed dataset is composed by four different groups of data collected using the Kinect. The first group of data has been obtained by recording 79 people with a frontal view, walking slowly, avoiding occlusions and with stretched arms (""Collaborative""). ",https://www.iit.it/research/lines/pattern-analysis-and-computer-vision/pavis-datasets/534-rgb-d-person-re-identification-dataset,https://www.iit.it/images/pavis/dataset/rgbd_reid_large.png,Re-identification with RGB-D sensors},"Barbosa, I. B., Cristani, M., Del Bue, A., Bazzani, L., & Murino, V. (2012, October). Re-identification with rgb-d sensors. In European Conference on Computer Vision (pp. 433-442). Springer, Berlin, Heidelberg.",EECV,Istituto Italiano di Tecnologia;University of Verona;Universit´e de Bourgogne,
143,UB KinFace Database,2011,"Ming Shao, Siyu Xia and Yun Fu",recognition,faces,kinship,images,landmark annotations,600,,"UB KinFace database is used to develop, test, and evaluate kinship verification and recognition algorithms. It comprises 600 images of 400 people which can be separated into 200 groups. Each group is composed of child, young parent and old parent images. Most of images in the database are real-world collections of public figures (celebrities and politicians) from Internet. ",http://www1.ece.neu.edu/~yunfu/research/Kinface/Kinface.htm,http://www1.ece.neu.edu/~yunfu/research/Kinface/Kinface_files/image002.jpg,Genealogical Face Recognition based on UB KinFace Database,"Ming Shao, Siyu Xia and Yun Fu, “Genealogical Face Recognition based on UB KinFace Database,” IEEE CVPR Workshop on Biometrics (BIOM), 2011.;Siyu Xia, Ming Shao and Yun Fu, “Kinship Verification through Transfer Learning,” International Joint Conferences on Artificial Intelligence (IJCAI), pp. 2539-2544, 2011.;Siyu Xia, Ming Shao, Jiebo Luo, and Yun Fu, “Understanding Kin Relationships in a Photo”, IEEE Transactions on Multimedia (T-MM), Volume: 14, Issue: 4, Page(s): 1046-1056, 2012.",CVPR;IJCAI,Northeastern University,
144,UPNA Head Pose Database,2016,"Mikel Ariz, José J. Bengoechea, Arantxa Villanueva, Rafael Cabeza",pose estimation;tracking,person,head,video,landmark annotations;joint positions/pose annotations,120,,"We have created a public database of videos for head tracking and pose estimation. The database consists of a set of 120 videos acquired with a standard webcam, corresponding to 10 different subjects (6 males and 4 females) and 12 videos each. ",http://gi4e.unavarra.es/databases/hpdb/,http://gi4e.unavarra.es/hpdb/imgs/sample_2.png,A novel 2D/3D database with automatic face annotation for head tracking and pose estimation,"Ariz, M., Bengoechea, J. J., Villanueva, A., & Cabeza, R. (2016). A novel 2d/3d database with automatic face annotation for head tracking and pose estimation. Computer Vision and Image Understanding, 148, 201-210.",,Public University of Navarra,
145,UPNA Synthetic Head Pose Database ,2017,"Andoni Larumbe, Mikel Ariz, José J. Bengoechea, Rubén Segura, Rafael Cabeza, Arantxa Villanueva",pose estimation;tracking,person,head,synthetic video,landmark annotations;joint positions/pose annotations,120,,"We have created a public database of videos for head tracking and pose estimation. The database is a synthetic replica of the UPNA Head Pose Database. Twelve videos per user have been thus generated, which include 6 guided-movement sequences and 6 freemovement sequences. ",http://gi4e.unavarra.es/databases/shpdb/,http://gi4e.unavarra.es/shpdb/imgs/sample_2_synthetic.png,Improved strategies for HPE employing learning-by-synthesis approaches,"Larumbe, A., Ariz, M., Bengoechea, J. J., Segura, R., Cabeza, R., & Villanueva, A. (2017). Improved Strategies for HPE Employing Learning-By-Synthesis Approaches. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 1545-1554).",ICCV,Public University of Navarra,
146,FDDB: Face Detection Data Set and Benchmark,2009,"Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller",recognition,faces,,images,identity annotations,13000,,"Labeled Faces in the Wild is a database of face photographs designed for studying the problem of unconstrained face recognition. The data set contains more than 13,000 images of faces collected from the web. Each face has been labeled with the name of the person pictured.",http://vis-www.cs.umass.edu/lfw/,,Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments.,"Huang, G. B., Ramesh, M., Berg, T., & Learned-Miller, E. (2007). Labeled faces in the wild: A database for studying face recognition in unconstrained environments (Vol. 1, No. 2, p. 3). Technical Report 07-49, University of Massachusetts, Amherst;Huang, G. B., & Learned-Miller, E. (2014). Labeled faces in the wild: Updates and new reporting procedures. Dept. Comput. Sci., Univ. Massachusetts Amherst, Amherst, MA, USA, Tech. Rep, 14-003.",ICCV,"University of Massachusetts, Amherst",
147,UTIRIS,2010,"Mahdi S. Hosseini, Babak N. Araabi and Hamid Soltanian-Zadeh",recognition;identification,eyes,,images,identity annotations,1540,,University of Tehran IRIS (UTIRIS) image repository is the first iris biometric databank registered in two distinct sessions of Visible Wavelength (VW) and Near InfraRed (NIR) imaging during 24-27th of June 2007. The database is constructed with 1540 images from 79 individuals from both right and left eyes demonstrated in 158 classes in total. The,https://utiris.wordpress.com/,,”Pigment Melanin: Pattern for Iris Recognition,"Hosseini, M. S., Araabi, B. N., & Soltanian-Zadeh, H. (2010). Pigment melanin: Pattern for iris recognition. IEEE Transactions on Instrumentation and Measurement, 59(4), 792-804.",,University of Tehran,
148,VIPSL Database ,2012,"Xinbo Gao, Nannan Wang, Dacheng Tao, and Xuelong Li",recognition;synthesis/generation,faces,sketches;retrieval,images,n/a,200,,"VIPSL Database is for research on face sketch-photo synthesis and recognition. It includes 200 persons from the FERET database, the FRAV2D database, and the Indian Face Database. For each face, there are five sketches drawn by five different artists based on the same photo taken in frontal pose, under normal lighting conditions, and with neutral expression.",http://www.ihitworld.com/index.php/vipsl-database,http://www.ihitworld.com/index.php/vipsl-database,Face Sketch-Photo Synthesis and Retrieval Using Sparse Representation,"Xinbo Gao, Nannan Wang, Dacheng Tao, and Xuelong Li. Face Sketch-Photo Synthesis and Retrieval Using Sparse Representation. IEEE Transactions on Circuits and Systems for Video Technology, vol. 22, no. 8, pp. 1213-1226, 2012.;Chunlei Peng, Nannan Wang, Xinbo Gao, and Jie Li. Face Recognition from Multiple Stylistic Sketches: Scenarios, Datasets, and Evaluation. European Conference on Computer Vision workshop on Visual Analysis of Sketches, 2016.",ECCV,Xidian University,
149,VT-KFER,2015,"Sherin Aly, Andrea Trubanova, Lynn Abbott, Susan White, and Amira Youssef",recognition,affect/expression,,3D video;video frames;images;depth images,category labels;valence/intensity annotations;landmark annotations;depth positions,1988,7,"VT-KFER includes both scripted and unscripted
expressions for 32 subjects. The scripted portion of the
dataset is composed of 1,956 sequences of RGB images
and depth maps for the six facial expressions in 3 poses,
frontal, right, and left. Our unscripted dataset includes 32 sequences, one for
each subject, that includes the subjects’ facial expressions
during a session of displaying the 53 IAPS pictures to them.",http://sufficiency.ece.vt.edu/VT-KFER/,http://sufficiency.ece.vt.edu/VT-KFER/Picture1.png,VT-KFER: A Kinect-based RGBD+Time Dataset for Spontaneous and Non-Spontaneous Facial Expression Recognition,"Aly, S., Trubanova, A., Abbott, L., White, S., & Youssef, A. (2015, May). VT-KFER: A Kinect-based RGBD+ time dataset for spontaneous and non-spontaneous facial expression recognition. In Biometrics (ICB), 2015 International Conference on (pp. 90-97). IEEE.",ICB,Virginia Tech;City of Scientific Research and Technological Applications;Alexandria University,
150,Facial Expression Research Group Database (FERG-DB) ,2016,"Aneja, Deepali and Colburn, Alex and Faigin, Gary and Shapiro, Linda and Mones, Barbara",recognition,affect/expression,,synthetic images,category labels,55767,7,Facial Expression Research Group Database (FERG-DB) is a database of stylized characters with annotated facial expressions. The database contains 55767 annotated face images of six stylized characters. The characters were modeled using the MAYA software and rendered out in 2D to create the images. ,http://grail.cs.washington.edu/projects/deepexpr/ferg-db.html,http://grail.cs.washington.edu/projects/deepexpr/sampleimages.jpg,Modeling Stylized Character Expressions via Deep Learning,"Aneja, D., Colburn, A., Faigin, G., Shapiro, L., & Mones, B. (2016, November). Modeling stylized character expressions via deep learning. In Asian Conference on Computer Vision(pp. 136-153). Springer, Cham.",ACCV,University of Washington;Zillow Group;Gage Academy of Art,
151,WIDER FACE: A Face Detection Benchmark ,2016,"Yang, Shuo and Luo, Ping and Loy, Chen Change and Tang, Xiaoou",detection,faces,,images,bounding boxes,32203,,"WIDER FACE dataset is a face detection benchmark dataset, of which images are selected from the publicly available WIDER dataset. We choose 32,203 images and label 393,703 faces with a high degree of variability in scale, pose and occlusion as depicted in the sample images. WIDER FACE dataset is organized based on 61 event classes. For each event class, we randomly select 40%/10%/50% data as training, validation and testing sets. ",http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/,http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/support/intro.jpg,WIDER FACE: A Face Detection Benchmark,"Yang, S., Luo, P., Loy, C. C., & Tang, X. (2016). Wider face: A face detection benchmark. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 5525-5533).",CVPR,The Chinese University of Hong Kong;Shenzhen Institutes of Advanced Technology,
152,YouTube Faces DB ,2011,"Lior Wolf, Tal Hassner and Itay Maoz",recognition,faces,,video,identity annotations,3425,,"YouTube Faces Database is a database of face videos designed for studying the problem of unconstrained face recognition in videos.
The data set contains 3,425 videos of 1,595 different people. All the videos were downloaded from YouTube. An average of 2.15 videos are available for each subject. The shortest clip duration is 48 frames, the longest clip is 6,070 frames, and the average length of a video clip is 181.3 frames. ",http://www.cs.tau.ac.il/~wolf/ytfaces/,http://www.cs.tau.ac.il/~wolf/ytfaces/logo.jpg,Face Recognition in Unconstrained Videos with Matched Background Similarity.,"Wolf, L., Hassner, T., & Maoz, I. (2011, June). Face recognition in unconstrained videos with matched background similarity. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on (pp. 529-534). IEEE.",CVPR,Tel-Aviv University;The Open University of Israel,
153,RENOIR,2014,"J. Anaya, A. Barbu",denoising/noise estimation,,,images,n/a,300,,A Dataset for Real Low-Light Image Noise Reduction,http://ani.stat.fsu.edu/~abarbu/Renoir.html,,A Dataset for Real Low-Light Image Noise Reduction,"Anaya, J., & Barbu, A. (2014). RENOIR-A Dataset for Real Low-Light Image Noise Reduction. arXiv preprint arXiv:1409.8230.",,Florida State University,
154,VGDB-2016,2016,Guilherme Folego and Otavio Gomes and Anderson Rocha,classification,object,painting,images,category labels,,,A dataset for identifying Van Gogh's paintings.,https://figshare.com/articles/From_Impressionism_to_Expressionism_Automatically_Identifying_Van_Gogh_s_Paintings/3370627,,From Impressionism to Expressionism: Automatically Identifying Van Gogh's Paintings,"Folego, G., Gomes, O., & Rocha, A. (2016, September). From impressionism to expressionism: Automatically identifying van Gogh's paintings. In Image Processing (ICIP), 2016 IEEE International Conference on (pp. 141-145). IEEE.",ICIAP,University of Campinas,
155,Archive of Many Outdoor Scenes (AMOS) ,2007,"Nathan Jacobs, Nathaniel Roman, Robert Pless",camera analysis,scenes,outdoor,images,geolocation labels,1128087180,,"The Archive of Many Outdoor Scenes (AMOS) dataset contains 1,128,087,180 images taken from 29945 webcams located around the world, the vast majority in the United States. Construction of AMOS began in March, 2006 and continues to this day.",http://amos.cse.wustl.edu/dataset,,Consistent Temporal Variations in Many Outdoor Scenes,"Nathan Jacobs, Nathaniel Roman, Robert Pless, ""Consistent Temporal Variations in Many Outdoor Scenes"", In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2007.;Nathan Jacobs, Scott Satkin, Nathaniel Roman, Richard Speyer, Robert Pless, ""Geolocating Static Cameras"", In IEEE International Conference on Computer Vision (ICCV), 2007.;Nathan Jacobs, Nathaniel Roman, Robert Pless, ""Toward Fully Automatic Geo-Location and Geo-Orientation of Static Outdoor Cameras"", In IEEE Workshop on Applications of Computer Vision (WACV), 2008.;Nathan Jacobs, Robert Pless, ""Calibrating and Using the Global Network of Outdoor Webcams"", In ACM/IEEE International Conference on Distributed Smart Cameras (ICDSC), 2009.;Nathan Jacobs, Richard Souvenir, Robert Pless, ""The Global Webcam Imaging Network"", In Applied Imagery Pattern Recognition Workshop (AIPR), 2009.;Nathan Jacobs, Walker Burgin, Richard Speyer, David Ross, Robert Pless, ""Adventures in Archiving and Using Three Years of Webcam Images"", In IEEE CVPR Workshop on Internet Vision, 2009.;Nathan Jacobs, Walker Burgin, Nick Fridrich, Austin Abrams, Kylia Miskell, Bobby H. Braswell, Andrew D. Richardson, Robert Pless, ""The Global Network of Outdoor Webcams: Properties and Applications"", In ACM International Conference on Advances in Geographic Information Systems (SIGSPATIAL GIS), 2009.;Austin Abrams, Nick Fridrich, Nathan Jacobs, Robert Pless, ""Participatory Integration of Live Webcams into GIS"", In International Conference on Computing for Geospatial Research and Applications (COM.GEO), 2010.;Austin Abrams, Emily Feder, Robert Pless. ""Exploratory Analysis of Time-Lapse Imagery with Fast Subset PCA"", in IEEE Workshop on Applications of Computer Vision (WACV) 2011.;Nathan Jacobs, Kylia Miskell, Robert Pless. ""Webcam Geo-localization using Aggregate Light Levels"", in IEEE Workshop on Applications of Computer Vision (WACV) 2011.;Austin Abrams, Robert Pless. ""Web-Accessible Geographic Integration and Calibration of Webcams"", in ACM Transactions on Multimedia Computing, Communication, and Applications 9(1); 8, 2013.;Austin Abrams, Christopher Hawley, Robert Pless. ""Heliometric Stereo: Shape from Sun Position"", in European Conference on Computer Vision (ECCV), 2012.;Austin Abrams, Kylia Miskell, Robert Pless. ""The Episolar Constraint: Monocular Shape from Shadow Correspondence"", in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013.;Calvin Murdock, Nathan Jacobs, Robert Pless. ""Webcam2Satellite: Estimating Cloud Maps from Webcam Imagery"", in IEEE Workshop on Applications of Computer Vision (WACV) 2013.",WACV;CVPR;ECCV;COM.GEO;GIS;AIPR;ICDSC;ICCV,Facebook;Washington University in St. Louis,
156,AutoDA (Automatic Dataset Augmentation),2017,"Yalong Bai, Kuiyuan Yang, Wei-Ying Ma, Tiejun Zhao",classification;recognition;description/captioning,object;images,,images,category labels;textual descriptions,12500000,1000,"AutoDA is constructed to augment existing dataset ILSVRC-2012. The AutoDA dataset contains 12.5 million images crawled from Web without URL or domain restriction. Since AutoDA is built to augment ILSVRC-2012, both datasets share the same 1000 categories.",https://auto-da.github.io/,,Automatic Dataset Augmentation,"Bai, Y., Yang, K., Ma, W. Y., & Zhao, T. (2017). Automatic Dataset Augmentation. arXiv preprint arXiv:1708.08201.",,Harbin Institute of Technology;DeepMotion;Bytedance,
157,Hyperspectral Database,2016,Boaz Arad;Ohad Ben-Shahar,imaging/image processing,images,hyperspectral imaging,images,category labels,201,,"Database of hyperspectral natural images acquired using a Specim PS Kappa DX4 hyperspectral camera and a rotary stage for spatial scanning. At this time 201 images were captured from a variety of urban (residential/commercial), suburban, rural, indoor and plant-life scenes but the database is designed to grow progressively. All images are 1392  × 1300 in spatial resolution and 519 spectral bands (400–1,000 nm at roughly 1.25 nm increments).",http://icvl.cs.bgu.ac.il/hyperspectral/,http://icvl.cs.bgu.ac.il/wp-content/uploads/2016/08/4cam_0411-1640-1-200x215.jpg,Sparse Recovery of Hyperspectral Signal from Natural RGB Images,"Arad, B., & Ben-Shahar, O. (2016, October). Sparse recovery of hyperspectral signal from natural RGB images. In European Conference on Computer Vision (pp. 19-34). Springer, Cham.",ECCV,Ben-Gurion University of the Negev,
158,CMP Facade Database,2013,Radim Tyle?cek and Radim S´ara,segmentation,city,,images,pixel-wise segmentation,606,12,"We present a dataset of facade images assembled at the Center for Machine Perception, which includes 606 rectified images of facades from various sources, which have been manually annotated. The facades are from different cities around the world and diverse architectural styles. ",http://cmp.felk.cvut.cz/~tylecr1/facade/,http://cmp.felk.cvut.cz/~tylecr1/facade/anot.png,Spatial Pattern Templates for Recognition of Objects with Regular Structure,"Tyle?ek, R., & Šára, R. (2013, September). Spatial pattern templates for recognition of objects with regular structure. In German Conference on Pattern Recognition (pp. 364-374). Springer, Berlin, Heidelberg.",GCPR,Czech Technical University in Prague,
159,Caltech-UCSD Birds-200-2011,2011,"Wah, C. and Branson, S. and Welinder, P. and Perona, P. and Belongie, S.",detection;localization;recognition,animal,bird,images,bounding boxes;localization line/shape coordinates;attribute annotations,11788,200,"CUB-200-2011 is an extended version of CUB-200, a challenging dataset of 200 bird species. The extended version roughly doubles the number of images per category and adds new part localization annotations. All images are annotated with bounding boxes, part locations, and attribute labels. Images and annotations were filtered by multiple users of Mechanical Turk",http://www.vision.caltech.edu/visipedia/CUB-200-2011.html,http://www.vision.caltech.edu/visipedia/collage.jpg,The Caltech-UCSD Birds-200-2011 Dataset,"Wah C., Branson S., Welinder P., Perona P., Belongie S. “The Caltech-UCSD Birds-200-2011 Dataset.” Computation & Neural Systems Technical Report, CNS-TR-2011-001.",,,"University of California, San Diego;California Institute of Technology"
160,DAQUAR - DAtaset for QUestion Answering on Real-world images,2014,"Malinowski, Mateusz and Fritz, Mario",recognition;description/captioning;segmentation,scenes,real-world;question-answer,images,textual descriptions;pixel-wise segmentation,1449,894," DAQUAR is a dataset of human question answer pairs about images, which manifests our vision on a Visual Turing Test.",https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/,https://www.mpi-inf.mpg.de/fileadmin/_processed_/b/1/csm_challenges_05a651d68d.png,A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input,"Malinowski, M., & Fritz, M. (2014). A multi-world approach to question answering about real-world scenes based on uncertain input. In Advances in neural information processing systems (pp. 1682-1690).",,Max Planck Institute for Informatics,
161,Video Face Recognition,2013,"Enrique G. Ortiz, Alan Wright, and Mubarak Shah",recognition,faces,movie,video frames;images,identity annotations,3585,,"We built our Movie Trailer Face Dataset using 113 movie trailers from YouTube of the 2010 release year that con tained celebrities present in our supplemented PublicFig+10 dataset. These videos were then processed to generate face tracks. The resulting dataset contains 3,585 face tracks, 63% consisting of unknown identities (not present in PubFig+10) and 37% 514 known.",http://enriquegortiz.com/wordpress/enriquegortiz/research/face-recognition/video-face-recognition/,http://enriquegortiz.com/wordpress/enriquegortiz/files/2013/02/trailers-1024x477.jpg,Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-based Classification,"E.G. Ortiz, A. Wright, and M. Shah. ""Face Recognition in Movie Trailers via Mean Sequence Spare Representation-based Classification"". IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013.",CVPR,University of Central Florida,
162,DSLR Photo Enhancement Dataset (DPED),2017,"Andrey Ignatov, Nikolay Kobyshev, Radu Timofte, Kenneth Vanhoey and Luc Van Gool",imaging/image processing,images,photo enhancement,images,n/a,22000,,"To tackle the general photo enhancement problem by mapping low-quality phone photos into photos captured by a professional DSLR camera, we introduce a large-scale DPED dataset that consists of photos taken synchronously in the wild by three smartphones and one DSLR camera. In total, over 22K photos were collected during 3 weeks, including 4549 photos from Sony smartphone, 5727 from iPhone and 6015 photos from BlackBerry; for each smartphone photo there is a corresponding photo from the Canon DSLR. The photos were taken during the daytime in a wide variety of places and in various illumination and weather conditions. The images were captured in automatic mode, we used default settings for all cameras throughout the whole collection procedure.",http://people.ee.ethz.ch/~ihnatova/,http://people.ee.ethz.ch/~ihnatova/assets/img/dped_quadro.png,DSLR-Quality Photos on Mobile Devices with Deep Convolutional Networks,"Ignatov, A., Kobyshev, N., Vanhoey, K., Timofte, R., & Van Gool, L. (2017, April). DSLR-quality photos on mobile devices with deep convolutional networks. In the IEEE Int. Conf. on Computer Vision (ICCV).",ICCV,ETH Zurich;ESAT - PSI,
163,Multispectral Imaging (MSI) datasets,2017,"Athanasios Zacharopoulos, Kostas Hatzigiannakis, Polykarpos Karamaoynas, Vassilis M. Papadakis, Michalis Andrianakis, Kristalia Melessanaki, Xenophon Zabulis",imaging/image processing,images,,images,landmark annotations,,,"Multispectral Imaging (MSI) datasets acquired using IRIS II which is a lightweight portable system comprising of a high resolution camera, a novel filter wheel able to interchange 23 filter positions and fast electronics and upon the same subject and viewing conditions. ",http://www.ics.forth.gr/cvrl/msi/,http://www.ics.forth.gr/cvrl/msi/figure01.png,A method for the registration of spectral images of paintings and its evaluation,"Zacharopoulos, A., Hatzigiannakis, K., Karamaoynas, P., Papadakis, V. M., Andrianakis, M., Melessanaki, K., & Zabulis, X. (2017). A method for the registration of spectral images of paintings and its evaluation. Journal of Cultural Heritage.",,Foundation for Research and Technology-Hellas;Delft University of Technology (Aerondt),
164,General 100 Dataset ,2016,"Chao Dong, Chen Change Loy, Xiaoou Tang",imaging/image processing,images,super-resolution,images,n/a,100,,"The General-100 dataset contains 100 bmp-format images (with no compression). The size of the newly introduced 100 images ranges from 710 × 704 (large) to 131 × 112 (small). They are all of good quality with clear edges but fewer smooth regions (e.g., sky and ocean), thus are very suitable for the SR training.",http://mmlab.ie.cuhk.edu.hk/projects/FSRCNN.html,,"Accelerating the Super-Resolution Convolutional Neural Network, in Proceedings of European Conference on Computer Vision ","Dong, C., Loy, C. C., & Tang, X. (2016, October). Accelerating the super-resolution convolutional neural network. In European Conference on Computer Vision (pp. 391-407). Springer, Cham.",ECCV,The Chinese University of Hong Kong,
165,Blur Dataset,2017,"Seungjun Nah, Tae Hyun Kim, Kyoung Mu Lee",imaging/image processing,,deblurring,images,n/a,3214,,Dataset of realistic blurry and sharp image pairs using a high-speed camera,https://github.com/SeungjunNah/DeepDeblur_release,https://github.com/SeungjunNah/DeepDeblur_release/raw/master/images/Istanbul_blur1.png,Deep Multi-Scale Convolutional Neural Network for Dynamic Scene Deblurring,"Nah, S., Kim, T. H., & Lee, K. M. (2017, July). Deep multi-scale convolutional neural network for dynamic scene deblurring. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (Vol. 2, No. 3).",CVPR,Seoul National University,
166,HPatches,2017,Vassileios Balntas and Karel Lenc and Andrea Vedaldi and Krystian Mikolajczyk,authentication/matching;recognition,images,,images,identity annotations;transformation/projection annotations,51,,Large-scale dataset of image sequences annotated with homographies. This is used to generate a patch-based benchmark suite for evaluating local image descriptors.,https://hpatches.github.io/,https://hpatches.github.io/assets/montage.png,HPatches: A benchmark and evaluation of handcrafted and learned local descriptors,"Balntas, V., Lenc, K., Vedaldi, A., & Mikolajczyk, K. (2017, April). HPatches: A benchmark and evaluation of handcrafted and learned local descriptors. In Conference on Computer Vision and Pattern Recognition (CVPR) (Vol. 4, No. 5, p. 6).",CVPR,Imperial College London;University of Oxford,
167,Hyperspectral images for spatial distributions of local illumination in natural scenes ,2015," D. H. Foster, S. M. C. Nascimento, and K. Amano",imaging/image processing,images,hyperspectral imaging,images,n/a,30,,"This set of hyperspectral radiance images has been taken from a study of scenes by Nascimento, Amano & Foster (2016). The set consists of 30 hyperspectral radiance images of natural scenes in which small neutral probe spheres were embedded to provide estimates of local illumination spectra. The 30 natural scenes are from the Minho region of Portugal and were acquired during late spring and summer of 2002 and 2003. The sky in most of the scenes was clear but in five it was overcast with cloud.",http://personalpages.manchester.ac.uk/staff/d.h.foster/Local_Illumination_HSIs/Local_Illumination_HSIs_2015.html,http://personalpages.manchester.ac.uk/staff/d.h.foster/Local_Illumination_HSIs/thumbnail_jpegs/Bom_Jesus_Red_flower.jpg,Spatial distributions of local illumination color in natural scenes,"Nascimento, S. M., Amano, K., & Foster, D. H. (2016). Spatial distributions of local illumination color in natural scenes. Vision research, 120, 39-44.",,University of Minho;University of Manchester,
168,ImageNet Large Scale Visual Recognition Challenges,2010,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg and Li Fei-Fei",classification;detection;localization,object;scenes,,images,category labels;bounding boxes;attribute annotations;localization line/shape coordinates,14197122,20000,"A large visual database designed for use in visual object recognition software research. Over 14 million URLs of images have been hand-annotated by ImageNet to indicate what objects are pictured; in at least one million of the images, bounding boxes are also provided.",www.image-net.org/,https://media.springernature.com/lw785/springer-static/image/art%3A10.1007%2Fs11263-015-0816-y/MediaObjects/11263_2015_816_Fig4_HTML.gif,ImageNet Large Scale Visual Recognition Challenge,"Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., ... & Berg, A. C. (2015). Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3), 211-252.",IJCV,Stanford University;University of Michigan;MIT;UNC Chapel Hill,
169,Multiply Distorted Image Database (MDID),2017,"W. Sun, F. Zhou, Q. M. Liao",imaging/image processing,images,distortion,images,n/a,1620,,The Multiply Distorted Image Database (MDID) is a database to evaluate image quality assessment (IQA) metrics on multiply distorted images. The database contains 20 reference images and 1600 distorted images. ,http://www.sz.tsinghua.edu.cn/labs/vipl/mdid.html,http://www.sz.tsinghua.edu.cn/labs/vipl/ref-mdid.png,MDID: a multiply distorted image database for image quality assessment,"Sun, W., Zhou, F., & Liao, Q. (2017). MDID: a multiply distorted image database for image quality assessment. Pattern Recognition, 61, 153-168.",,Tsinghua University,
170,NPRgeneral,2016,David Mould and Paul L. Rosin,imaging/image processing,images,stylization,images,attribute annotations,20,,A benchmark image set to which image stylization methods can be applied.,http://gigl.scs.carleton.ca/benchmark_npr_general,http://gigl.scs.carleton.ca/sites/default/files/angel1024_thumb.jpg,A benchmark image set for evaluating stylization,"Mould, D., & Rosin, P. L. (2016, May). A benchmark image set for evaluating stylization. In Proceedings of the Joint Symposium on Computational Aesthetics and Sketch Based Interfaces and Modeling and Non-Photorealistic Animation and Rendering (pp. 11-20). Eurographics Association.",,Carleton University;Cardiff University,
171,NYU Symmetry Database ,2016,M. Cicconet and V. Birodkar and M. Lund and M. Werman and D. Geiger,recognition,images,symmetry,images,localization line/shape coordinates,239,,Database for 2D symmetry detection. The mirror symmetry database contains 176 single-symmetry and 63 multiple-symmetry images (.png files) with accompanying ground-truth annotations (.mat files). ,http://symmetry.cs.nyu.edu/,http://symmetry.cs.nyu.edu/I140.png,A convolutional approach to reflection symmetry,"Cicconet, M., Birodkar, V., Lund, M., Werman, M., & Geiger, D. (2017). A convolutional approach to reflection symmetry. Pattern Recognition Letters, 95, 44-50.",,New York University,
172,Audio-Visual Vehicle (AVV) Dataset,2012,"Wang, T. and Zhu, Z. (",recognition;classification,vehicle,,images;audio,category labels;localization line/shape coordinates,961,,"Ground level moving vehicle detection and classification under various challenging conditions (occlusions, motion blur, various perspective views).",http://vcipl-okstate.org/pbvs/bench/,,Real time vehicle detection and reconstruction for improving classification,"Wang, T., & Zhu, Z. (2012, January). Real time moving vehicle detection and reconstruction for improving classification. In Applications of Computer Vision (WACV), 2012 IEEE Workshop on (pp. 497-502). IEEE.",WACV,CUNY;The City College of New York,
173,CSIR-CSIO Moving Object Thermal Infrared Imagery Dataset (MOTIID),2013,"Akula, A., Ghosh, R., Kumar, S., & Sardana, H. K. ",detection,object;vehicle;pedestrian,infrared,video;video frames;images,temporal bounding boxes,18,,"Moving object (Pedestrian, Vehicle, etc.) detection in thermal infrared imagery ",http://vcipl-okstate.org/pbvs/bench/,http://vcipl-okstate.org/pbvs/bench/images/Data-09-logo.jpg,Moving target detection in thermal infrared imagery using spatiotemporal information,"Akula, A., Ghosh, R., Kumar, S., & Sardana, H. K. (2013). Moving target detection in thermal infrared imagery using spatiotemporal information. JOSA A, 30(8), 1492-1501;Akula, A., Khanna, N., Ghosh, R., Kumar, S., Das, A., & Sardana, H. K. (2014). Adaptive contour-based statistical background subtraction method for moving target detection in infrared video sequences. Infrared Physics & Technology, 63, 103-109.",,Academy of Scientific and Innovative Research (AcSIR);Central Scientific Instruments Organisation (CSIR-CSIO),
174,Pedestrian Infrared/visible Stereo Video Dataset,2014,"Bilodeau, G.-A., Torabi, A., St-Charles, P.-L., Riahi, D.",authentication/matching;recognition,pedestrian,infrared,video;video frames;images,localization line/shape coordinates;attribute annotations,206,,Registration of pedestrian at close range in infrared/visible stereo videos.,http://vcipl-okstate.org/pbvs/bench/,http://vcipl-okstate.org/pbvs/bench/images/Data-10-logo.jpg,Thermal–visible registration of human silhouettes: A similarity measure performance evaluation,"Bilodeau, G. A., Torabi, A., St-Charles, P. L., & Riahi, D. (2014). Thermal–visible registration of human silhouettes: A similarity measure performance evaluation. Infrared Physics & Technology, 64, 79-86.",,Ecole Polytechnique de Montreal;Universite de Montreal,
175,Thermal Infrared Video Benchmark for Visual Analysis,2014,"Wu, Z., Fuller, N., Theriault, D., & Betke, M.",detection;tracking,object;person;pedestrian;animal,infrared,video;video frames;images,bounding boxes;tracking annotations,60000,,"Object detection, counting and tracking with single/multiple views in infrared videos.",http://vcipl-okstate.org/pbvs/bench/,http://vcipl-okstate.org/pbvs/bench/images/benchmark11.png,A thermal infrared video benchmark for visual analysis. ,"Wu, Z., Fuller, N., Theriault, D., & Betke, M. (2014, June). A thermal infrared video benchmark for visual analysis. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2014 IEEE Conference on (pp. 201-208). IEEE.",CVPR,The Mathworks Inc.;Boston University,
176,Maritime Imagery in the Visible and Infrared Spectrums,2015,"Zhang, M. M., Choi, J., Daniilidis, K., Wolf, M. T., & Kanan, C. ",classification,object,ships,images,category labels,2865,6,VAIS contains simultaneously acquired unregistered thermal and visible images of ships acquired from piers. It is suitable for object classification research. ,http://vcipl-okstate.org/pbvs/bench/,http://vcipl-okstate.org/pbvs/bench/images/Data-12-logo.jpg,VAIS: A Dataset for Recognizing Maritime Imagery in the Visible and Infrared Spectrums,"Zhang, M. M., Choi, J., Daniilidis, K., Wolf, M. T., & Kanan, C. (2015). Vais: A dataset for recognizing maritime imagery in the visible and infrared spectrums. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (pp. 10-16).",CVPR,University of Pennsylvania;Gwangju Institute of Science and Technology;California Institute of Technology,
177,"Stanford Street View Image, Pose, and 3D Cities Dataset ",2016,"Amir R Zamir, Tilman Wekel, Pulkit Agrawal, Jitendra Malik and Silvio Savarese
",pose estimation;representation,object;city,,images,geolocation labels;joint positions/pose annotations,25000000,,"We contribute a large-scale dataset composed of object-centric Street View scenes along with point correspondences and camera pose information. We collected the dataset by integrating Street View images, their metadata, and large-scale geo-registered 3D building models scraped on the web. ",http://3drepresentation.stanford.edu/,http://3drepresentation.stanford.edu/static/images/examples/10/examples-046.jpg,Generic 3D Representation via Pose Estimation and Matching,"Zamir, A. R., Wekel, T., Agrawal, P., Wei, C., Malik, J., & Savarese, S. (2016, October). Generic 3d representation via pose estimation and matching. In European Conference on Computer Vision (pp. 535-553). Springer, Cham.",ECCV,"Stanford University;University of California, Berkeley",
178,TESTIMAGES,2014,N. Asuni and A. Giachetti,imaging/image processing,images,displays,images,n/a,2000000,,"The TESTIMAGES archive is a huge and free collection of sample images designed for analysis and quality assessment of different kinds of displays (i.e. monitors, televisions and digital cinema projectors) and image processing techniques. The archive includes more than 2 million images originally acquired and divided in three different categories: SAMPLING and SAMPLING_PATTERNS (aimed at testing resampling algorithms), COLOR (aimed at testing color rendering on different displays) and PATTERNS (aimed at testing the rendering of standard geometrical patterns).",https://testimages.org/,https://testimages.org/img/testimages_screenshot.jpg,TESTIMAGES: a large-scale archive for testing visual devices and basic image processing algorithms,"Asuni, N., & Giachetti, A. (2014, September). TESTIMAGES: a Large-scale Archive for Testing Visual Devices and Basic Image Processing Algorithms. In Eurographics Italian Chapter Conference (pp. 63-70).;Asuni, N., & Giachetti, A. (2013). TESTIMAGES: A Large Data Archive For Display and Algorithm Testing. Journal of Graphics Tools, 17(4), 113-125.",EICC,University of Verona,
179,Time-Lapse Hyperspectral Radiance Images of Natural Scenes,2015,"D. H. Foster, S. M. C. Nascimento, and K. Amano",imaging/image processing,images,hyperspectral imaging,images,n/a,,,"Sequences of hyperspectral radiance images taken from a study by Foster, Amano, and Nascimento (2016) of scenes undergoing natural illumination changes.",http://personalpages.manchester.ac.uk/staff/d.h.foster/Time-Lapse_HSIs/Time-Lapse_HSIs_2015.html,http://personalpages.manchester.ac.uk/staff/d.h.foster/Time-Lapse_HSIs/nogueiro/thumbnails/Nogueiro_sequence_RGB_1140.jpg,Time-lapse ratios of cone excitations in natural scenes,"Foster, D. H., Amano, K., & Nascimento, S. M. (2016). Time-lapse ratios of cone excitations in natural scenes. Vision research, 120, 45-60.",,University of Manchester;University of Minho,
180,Tiny Images Dataset ,2008,"Antonio Torralba, Rob Fergus and William T. Freeman",recognition;classification;detection;segmentation,images,Wordnet,images,category labels;bounding boxes;pixel-wise segmentation;textual descriptions,79302017,75062,"Tiny Images dataset consists of 79,302,017 images, each being a 32x32 color image. Each image is loosely labeled with one of the
75,062 non-abstract nouns in English, as listed in the Wordnet
lexical database. Hence the image database gives a comprehensive
coverage of all object categories and scenes",http://groups.csail.mit.edu/vision/TinyImages/,http://groups.csail.mit.edu/vision/TinyImages/mosaicPaper1024.jpg,80 million tiny images: A large data set for nonparametric object and scene recognition,"Torralba, A., Fergus, R., & Freeman, W. T. (2008). 80 million tiny images: A large data set for nonparametric object and scene recognition. IEEE transactions on pattern analysis and machine intelligence, 30(11), 1958-1970.",,MIT,
181,Visual Dialog,2017,"Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José M. F. Moura, Devi Parikh and Dhruv Batra",description/captioning,speech/language,dialog;HCI,images,textual descriptions,120000,,Human dialog on 120k COCO images.,https://visualdialog.org/,https://visualchatbot.cloudcv.org/#,Visual dialog,"Das, A., Kottur, S., Gupta, K., Singh, A., Yadav, D., Moura, J. M., ... & Batra, D. (2017, July). Visual dialog. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (Vol. 2).",CVPR;ICCV;HCOMP,Georgia Institute of Technology;Carnegie Mellon University;UC Berkeley;Virginia Tech,
182,Visual Question Answering (VQA),2017,"Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh",description/captioning,speech/language,,images,textual descriptions,265016,,"Visual Question Answering (VQA) balances the popular VQA dataset by collecting complementary images such that every question in the balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. The dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs.",http://visualqa.org/,http://visualqa.org/static/img/vqa_examples.jpg,Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering,"Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., & Parikh, D. (2017, July). Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. In CVPR(Vol. 1, No. 6, p. 9).",CVPR,Virginia Tech;Army Research Laboratory;Georgia Institute of Technology,
183,VQA-Human Attention (VQA-HAT) Dataset,2016,"Abhishek Das, Harsh Agrawal, C. Lawrence Zitnick, Devi Parikh, Dhruv Batra",gaze estimation,speech/language,attention;attention maps,images,gaze fixation annotations,62597,,VQA-HAT (Human ATtention) dataset evaluates attention maps generated by state-of-the-art VQA models against human attention both qualitatively (via visualizations) and quantitatively (via rank-order correlation). ,https://computing.ece.vt.edu/~abhshkdz/vqa-hat/,https://computing.ece.vt.edu/~abhshkdz/static/img/vqa-hat/att_comparison.jpg,Human attention in visual question answering: Do humans and deep networks look at the same regions?,"Das, A., Agrawal, H., Zitnick, L., Parikh, D., & Batra, D. (2017). Human attention in visual question answering: Do humans and deep networks look at the same regions?. Computer Vision and Image Understanding, 163, 90-100.",EMNLP;ICML,Virginia Tech;Facebook AI Research,
184,Wild Web tampered image dataset ,2015,"Markos Zampoglou, Symeon Papadopoulous and Yiannis Kompatsiaris",authentication/matching,,tampering,images,masks,80,,"The Wild Web dataset is a very large collection of forgeries collected from various Web and social media sources, accompanied by ground truth binary masks localizing the forgery, and by the image sources that were used to perform the forgery, wherever these are avaiable.",http://mklab.iti.gr/project/wild-web-tampered-image-dataset,,Detecting image splicing in the wild (web),"Zampoglou, M., Papadopoulos, S., & Kompatsiaris, Y. (2015, June). Detecting image splicing in the wild (web). In Multimedia & Expo Workshops (ICMEW), 2015 IEEE International Conference on (pp. 1-6). IEEE.",ICME,Centre for Research and Technology Hellas,
185,YFCC100M: The New Data in Multimedia Research ,2016,"Sebastian Kalkowski, Christian Schulze, Andreas Dengel, Damian Borth",recognition;classification,object;scenes,,images,category labels,100000000,,Dataset of 100 million photos and videos.,http://projects.dfki.uni-kl.de/yfcc100m/,http://projects.dfki.uni-kl.de/yfcc100m/imgs/browser_flow_small.png,Real-time Analysis and Visualization of the YFCC100m Dataset,"Kalkowski, S., Schulze, C., Dengel, A., & Borth, D. (2015, October). Real-time analysis and visualization of the YFCC100M dataset. In Proceedings of the 2015 Workshop on Community-Organized Multimodal Mining: Opportunities for Novel Solutions (pp. 25-30). ACM.",,University of Kaiserslautern;German Research Center for Artificial Intelligence (DFKI),
